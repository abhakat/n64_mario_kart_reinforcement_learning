{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 10: Training at Scale - Full Pipeline\n",
    "\n",
    "In this final checkpoint, we bring everything together into a **production-ready training pipeline**. You'll learn how to train RL agents at scale with proper monitoring, checkpointing, and evaluation.\n",
    "\n",
    "## Learning Objectives\n",
    "- Configure and run large-scale PPO training\n",
    "- Monitor training with TensorBoard\n",
    "- Implement custom callbacks for detailed logging\n",
    "- Evaluate and visualize trained policies\n",
    "- Understand common failure modes and debugging strategies\n",
    "- Apply the pipeline to Mario Kart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install \"gymnasium[classic-control,box2d]\" stable-baselines3 tensorboard torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Check with Memory Info\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Hardware Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Available: True\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Memory info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = props.total_memory / 1e9\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
    "    \n",
    "    # Current memory usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    cached = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"Currently Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Currently Cached: {cached:.2f} GB\")\n",
    "    \n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(f\"MPS (Apple Silicon) Available: True\")\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    print(f\"CUDA Available: False\")\n",
    "    print(f\"Training will use CPU (slower but works)\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\nSelected Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Metrics to Monitor\n",
    "\n",
    "TensorBoard is essential for understanding training progress. Here are the key metrics:\n",
    "\n",
    "| Metric | Description | What to Look For | Warning Signs |\n",
    "|--------|-------------|------------------|---------------|\n",
    "| `rollout/ep_rew_mean` | Mean episode reward | Steady increase over time | Flat, declining, or oscillating |\n",
    "| `rollout/ep_len_mean` | Mean episode length | Task-dependent | Sudden changes may indicate problems |\n",
    "| `train/policy_loss` | Policy network loss | Stable, relatively small | Large spikes, divergence |\n",
    "| `train/value_loss` | Value network loss | Decreasing over time | Increasing or exploding |\n",
    "| `train/entropy_loss` | Exploration entropy | Gradual decrease | Too fast = premature convergence |\n",
    "| `train/approx_kl` | KL divergence | Small (< 0.02) | Large values = unstable updates |\n",
    "| `train/clip_fraction` | PPO clipping rate | 0.1 - 0.3 typical | Very high = aggressive updates |\n",
    "| `train/learning_rate` | Current LR | As scheduled | N/A |\n",
    "| `time/fps` | Training speed | Stable | Decreasing = memory issues |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Failure Modes\n",
    "\n",
    "Understanding what can go wrong helps you debug faster.\n",
    "\n",
    "### 1. Reward Hacking\n",
    "**Symptoms**: High reward but poor qualitative behavior  \n",
    "**Causes**: Reward function doesn't capture true objective  \n",
    "**Solutions**: Redesign reward, add constraints, use auxiliary metrics\n",
    "\n",
    "### 2. Catastrophic Forgetting\n",
    "**Symptoms**: Performance drops after initial improvement  \n",
    "**Causes**: Learning rate too high, environment distribution shift  \n",
    "**Solutions**: Lower learning rate, use replay buffer, curriculum learning\n",
    "\n",
    "### 3. Policy Collapse\n",
    "**Symptoms**: Agent always takes same action, entropy drops to zero  \n",
    "**Causes**: Entropy coefficient too low, reward too sparse  \n",
    "**Solutions**: Increase entropy bonus, add reward shaping\n",
    "\n",
    "### 4. Exploration Failure\n",
    "**Symptoms**: Reward plateaus early, agent stuck in local optimum  \n",
    "**Causes**: Insufficient exploration, environment too hard  \n",
    "**Solutions**: Increase entropy, use intrinsic motivation, curriculum\n",
    "\n",
    "### 5. Hyperparameter Sensitivity\n",
    "**Symptoms**: Results vary wildly between runs  \n",
    "**Causes**: Unstable learning dynamics, poor hyperparameters  \n",
    "**Solutions**: Use multiple seeds, hyperparameter search, robust defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Strategies\n",
    "\n",
    "When training isn't working, try these approaches:\n",
    "\n",
    "### 1. Visualize Episodes\n",
    "- Record videos of the agent's behavior\n",
    "- Look for unexpected patterns\n",
    "- Compare early vs late training\n",
    "\n",
    "### 2. Check Reward Distribution\n",
    "- Plot reward histograms\n",
    "- Identify reward spikes or anomalies\n",
    "- Verify reward scale is appropriate\n",
    "\n",
    "### 3. Monitor Gradients\n",
    "- Watch for gradient explosions/vanishing\n",
    "- Check gradient norms in TensorBoard\n",
    "\n",
    "### 4. Compare to Baselines\n",
    "- Random policy performance\n",
    "- Simple heuristic policies\n",
    "- Published results (if available)\n",
    "\n",
    "### 5. Ablation Studies\n",
    "- Remove components one at a time\n",
    "- Identify which parts are essential\n",
    "- Simplify until something works\n",
    "\n",
    "### 6. Sanity Checks\n",
    "- Can the agent solve a trivial version?\n",
    "- Are observations/actions normalized correctly?\n",
    "- Is the environment deterministic for debugging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Dictionary\n",
    "\n",
    "CONFIG = {\n",
    "    # Environment\n",
    "    'env_name': 'LunarLander-v3',\n",
    "    \n",
    "    # Training\n",
    "    'total_timesteps': 500_000,\n",
    "    'n_envs': 8,  # Number of parallel environments\n",
    "    \n",
    "    # Evaluation\n",
    "    'eval_freq': 10_000,  # Evaluate every N timesteps\n",
    "    'n_eval_episodes': 10,\n",
    "    \n",
    "    # Checkpointing\n",
    "    'save_freq': 50_000,  # Save checkpoint every N timesteps\n",
    "    \n",
    "    # Directories\n",
    "    'log_dir': './logs/ppo_lunarlander',\n",
    "    'model_dir': './models/ppo_lunarlander',\n",
    "    'tensorboard_log': './tensorboard/ppo_lunarlander',\n",
    "    \n",
    "    # PPO Hyperparameters\n",
    "    'learning_rate': 3e-4,\n",
    "    'n_steps': 2048,\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 10,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_range': 0.2,\n",
    "    'ent_coef': 0.01,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    BaseCallback,\n",
    "    CheckpointCallback,\n",
    "    EvalCallback,\n",
    "    CallbackList\n",
    ")\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DetailedLoggingCallback: Custom Callback for Episode Tracking\n",
    "\n",
    "class DetailedLoggingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for detailed episode logging.\n",
    "    \n",
    "    Tracks:\n",
    "    - Episode rewards and lengths\n",
    "    - Running statistics\n",
    "    - Periodic progress reports\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: int = 0, log_freq: int = 100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            verbose: Verbosity level\n",
    "            log_freq: Print stats every N episodes\n",
    "        \"\"\"\n",
    "        super().__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        \n",
    "        # Episode tracking\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        # Timing\n",
    "        self.start_time = None\n",
    "    \n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"Called at the start of training.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\nTraining started at {time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"Target: {self.locals['total_timesteps']:,} timesteps\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Called after each environment step.\n",
    "        \n",
    "        Returns:\n",
    "            True to continue training, False to stop\n",
    "        \"\"\"\n",
    "        # Check for completed episodes\n",
    "        for info in self.locals.get('infos', []):\n",
    "            if 'episode' in info:\n",
    "                ep_reward = info['episode']['r']\n",
    "                ep_length = info['episode']['l']\n",
    "                \n",
    "                self.episode_rewards.append(ep_reward)\n",
    "                self.episode_lengths.append(ep_length)\n",
    "                self.episode_count += 1\n",
    "                \n",
    "                # Periodic logging\n",
    "                if self.episode_count % self.log_freq == 0:\n",
    "                    self._log_progress()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _log_progress(self) -> None:\n",
    "        \"\"\"Print progress statistics.\"\"\"\n",
    "        recent_rewards = self.episode_rewards[-self.log_freq:]\n",
    "        recent_lengths = self.episode_lengths[-self.log_freq:]\n",
    "        \n",
    "        avg_reward = np.mean(recent_rewards)\n",
    "        std_reward = np.std(recent_rewards)\n",
    "        avg_length = np.mean(recent_lengths)\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        eps_per_sec = self.episode_count / elapsed\n",
    "        \n",
    "        print(f\"Episode {self.episode_count:>6d} | \"\n",
    "              f\"Reward: {avg_reward:>8.2f} +/- {std_reward:<6.2f} | \"\n",
    "              f\"Length: {avg_length:>6.1f} | \"\n",
    "              f\"EPS: {eps_per_sec:.1f}\")\n",
    "    \n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"Called at the end of training.\"\"\"\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Training completed!\")\n",
    "        print(f\"Total episodes: {self.episode_count:,}\")\n",
    "        print(f\"Total time: {elapsed/60:.2f} minutes\")\n",
    "        print(f\"Final avg reward (last 100): {np.mean(self.episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "print(\"DetailedLoggingCallback defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vectorized Training Environment with VecMonitor\n",
    "\n",
    "def make_env(env_name: str, rank: int = 0):\n",
    "    \"\"\"\n",
    "    Create a function that returns a monitored environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Gymnasium environment ID\n",
    "        rank: Unique identifier for parallel envs\n",
    "    \n",
    "    Returns:\n",
    "        Callable that creates the environment\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Create vectorized environment\n",
    "# Note: SubprocVecEnv runs envs in separate processes (faster but more memory)\n",
    "# DummyVecEnv runs in single process (slower but simpler for debugging)\n",
    "\n",
    "print(f\"Creating {CONFIG['n_envs']} parallel environments...\")\n",
    "\n",
    "# Use SubprocVecEnv for speed (comment out and use DummyVecEnv for debugging)\n",
    "try:\n",
    "    train_envs = SubprocVecEnv(\n",
    "        [make_env(CONFIG['env_name'], i) for i in range(CONFIG['n_envs'])]\n",
    "    )\n",
    "    print(\"Using SubprocVecEnv (multiprocessing)\")\n",
    "except Exception as e:\n",
    "    print(f\"SubprocVecEnv failed ({e}), falling back to DummyVecEnv\")\n",
    "    train_envs = DummyVecEnv(\n",
    "        [make_env(CONFIG['env_name'], i) for i in range(CONFIG['n_envs'])]\n",
    "    )\n",
    "\n",
    "# Wrap with VecMonitor for automatic episode statistics\n",
    "train_envs = VecMonitor(train_envs)\n",
    "\n",
    "print(f\"Training environment ready!\")\n",
    "print(f\"  Observation space: {train_envs.observation_space}\")\n",
    "print(f\"  Action space: {train_envs.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Evaluation Environment\n",
    "\n",
    "# Separate environment for evaluation (single env with rendering)\n",
    "eval_env = gym.make(CONFIG['env_name'], render_mode='rgb_array')\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "print(f\"Evaluation environment created: {CONFIG['env_name']}\")\n",
    "print(f\"  Render mode: rgb_array (for visualization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Callbacks\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['log_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['model_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['tensorboard_log'], exist_ok=True)\n",
    "\n",
    "print(\"Setting up callbacks...\")\n",
    "\n",
    "# 1. Checkpoint Callback: Save model periodically\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=CONFIG['save_freq'] // CONFIG['n_envs'],  # Adjust for parallel envs\n",
    "    save_path=CONFIG['model_dir'],\n",
    "    name_prefix='ppo_checkpoint',\n",
    "    verbose=1\n",
    ")\n",
    "print(f\"  - Checkpoint: Every {CONFIG['save_freq']:,} steps -> {CONFIG['model_dir']}\")\n",
    "\n",
    "# 2. Evaluation Callback: Evaluate and save best model\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=CONFIG['model_dir'],\n",
    "    log_path=CONFIG['log_dir'],\n",
    "    eval_freq=CONFIG['eval_freq'] // CONFIG['n_envs'],\n",
    "    n_eval_episodes=CONFIG['n_eval_episodes'],\n",
    "    deterministic=True,\n",
    "    verbose=1\n",
    ")\n",
    "print(f\"  - Evaluation: Every {CONFIG['eval_freq']:,} steps ({CONFIG['n_eval_episodes']} episodes)\")\n",
    "\n",
    "# 3. Custom Logging Callback\n",
    "logging_callback = DetailedLoggingCallback(verbose=1, log_freq=100)\n",
    "print(f\"  - Custom logging: Every 100 episodes\")\n",
    "\n",
    "# Combine all callbacks\n",
    "callbacks = CallbackList([checkpoint_callback, eval_callback, logging_callback])\n",
    "\n",
    "print(\"\\nCallbacks ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO Model with TensorBoard Logging\n",
    "\n",
    "model = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=train_envs,\n",
    "    verbose=1,\n",
    "    tensorboard_log=CONFIG['tensorboard_log'],\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    n_steps=CONFIG['n_steps'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    n_epochs=CONFIG['n_epochs'],\n",
    "    gamma=CONFIG['gamma'],\n",
    "    gae_lambda=CONFIG['gae_lambda'],\n",
    "    clip_range=CONFIG['clip_range'],\n",
    "    ent_coef=CONFIG['ent_coef'],\n",
    "    \n",
    "    # Device\n",
    "    device='auto',  # Automatically select GPU if available\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.policy.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.policy.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\nPPO Model Created\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Policy: MlpPolicy\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Device: {model.device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard Integration\n",
    "# Run this cell to start TensorBoard inline\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {CONFIG['tensorboard_log']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cell: 500k Timesteps with Progress Bar\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(f\"Target: {CONFIG['total_timesteps']:,} timesteps\")\n",
    "print(f\"Estimated time: 5-15 minutes (depending on hardware)\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "model.learn(\n",
    "    total_timesteps=CONFIG['total_timesteps'],\n",
    "    callback=callbacks,\n",
    "    progress_bar=True,\n",
    "    tb_log_name='PPO'\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"Total time: {training_time/60:.2f} minutes\")\n",
    "print(f\"Steps per second: {CONFIG['total_timesteps']/training_time:.0f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save final model\n",
    "final_model_path = f\"{CONFIG['model_dir']}/ppo_final\"\n",
    "model.save(final_model_path)\n",
    "print(f\"\\nFinal model saved to: {final_model_path}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Analysis Plots\n",
    "\n",
    "# Get episode data from our custom callback\n",
    "episode_rewards = logging_callback.episode_rewards\n",
    "episode_lengths = logging_callback.episode_lengths\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Episode Rewards Over Time\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(episode_rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "\n",
    "# Rolling average\n",
    "window = min(100, len(episode_rewards) // 10) or 1\n",
    "if len(episode_rewards) >= window:\n",
    "    rolling_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(episode_rewards)), rolling_avg, \n",
    "             color='red', linewidth=2, label=f'Rolling Avg ({window})')\n",
    "\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Episode Rewards During Training')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Reward Distribution Histogram\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(episode_rewards, bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=np.mean(episode_rewards), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(episode_rewards):.1f}')\n",
    "ax2.axvline(x=np.median(episode_rewards), color='orange', linestyle='--',\n",
    "            label=f'Median: {np.median(episode_rewards):.1f}')\n",
    "ax2.set_xlabel('Reward')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Reward Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Episode Lengths\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(episode_lengths, alpha=0.3, color='purple', label='Episode Length')\n",
    "\n",
    "if len(episode_lengths) >= window:\n",
    "    rolling_len = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\n",
    "    ax3.plot(range(window-1, len(episode_lengths)), rolling_len,\n",
    "             color='darkred', linewidth=2, label=f'Rolling Avg ({window})')\n",
    "\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Steps')\n",
    "ax3.set_title('Episode Lengths During Training')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Learning Curve (by training phase)\n",
    "ax4 = axes[1, 1]\n",
    "n_phases = 5\n",
    "phase_size = len(episode_rewards) // n_phases\n",
    "if phase_size > 0:\n",
    "    phase_means = []\n",
    "    phase_stds = []\n",
    "    phases = []\n",
    "    \n",
    "    for i in range(n_phases):\n",
    "        start_idx = i * phase_size\n",
    "        end_idx = start_idx + phase_size\n",
    "        phase_data = episode_rewards[start_idx:end_idx]\n",
    "        phase_means.append(np.mean(phase_data))\n",
    "        phase_stds.append(np.std(phase_data))\n",
    "        phases.append(f\"Phase {i+1}\\n({start_idx}-{end_idx})\")\n",
    "    \n",
    "    x_pos = np.arange(len(phases))\n",
    "    bars = ax4.bar(x_pos, phase_means, yerr=phase_stds, capsize=5,\n",
    "                   color=['#ff9999', '#ffcc99', '#ffff99', '#99ff99', '#99ccff'],\n",
    "                   edgecolor='black')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(phases, fontsize=9)\n",
    "    ax4.set_ylabel('Mean Reward')\n",
    "    ax4.set_title('Learning Progress by Phase')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['log_dir']}/training_analysis.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(f\"  Total Episodes: {len(episode_rewards)}\")\n",
    "print(f\"  Mean Reward: {np.mean(episode_rewards):.2f} +/- {np.std(episode_rewards):.2f}\")\n",
    "print(f\"  Max Reward: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"  Min Reward: {np.min(episode_rewards):.2f}\")\n",
    "print(f\"  Mean Episode Length: {np.mean(episode_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation: 20 Episodes with Best Tracking\n",
    "\n",
    "print(\"Loading best model for evaluation...\")\n",
    "\n",
    "# Load the best model (saved by EvalCallback)\n",
    "best_model_path = f\"{CONFIG['model_dir']}/best_model\"\n",
    "if os.path.exists(f\"{best_model_path}.zip\"):\n",
    "    best_model = PPO.load(best_model_path)\n",
    "    print(f\"Loaded: {best_model_path}\")\n",
    "else:\n",
    "    print(\"Best model not found, using final model\")\n",
    "    best_model = model\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 20\n",
    "eval_rewards = []\n",
    "eval_lengths = []\n",
    "best_reward = -float('inf')\n",
    "best_frames = None\n",
    "all_episode_frames = []\n",
    "\n",
    "print(f\"\\nRunning {n_eval_episodes} evaluation episodes...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for ep in range(n_eval_episodes):\n",
    "    obs, info = eval_env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    frames = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from policy (deterministic for evaluation)\n",
    "        action, _ = best_model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Step environment\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        # Capture frame for visualization\n",
    "        frame = eval_env.render()\n",
    "        if frame is not None:\n",
    "            frames.append(frame)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "    \n",
    "    eval_rewards.append(episode_reward)\n",
    "    eval_lengths.append(episode_length)\n",
    "    \n",
    "    # Track best episode\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "        best_frames = frames.copy()\n",
    "        best_episode = ep + 1\n",
    "    \n",
    "    # Store frames for visualization\n",
    "    all_episode_frames.append(frames)\n",
    "    \n",
    "    print(f\"Episode {ep+1:2d}: Reward = {episode_reward:>8.2f}, Length = {episode_length:>4d}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nFinal Evaluation Results:\")\n",
    "print(f\"  Mean Reward: {np.mean(eval_rewards):.2f} +/- {np.std(eval_rewards):.2f}\")\n",
    "print(f\"  Max Reward: {np.max(eval_rewards):.2f} (Episode {np.argmax(eval_rewards)+1})\")\n",
    "print(f\"  Min Reward: {np.min(eval_rewards):.2f}\")\n",
    "print(f\"  Mean Length: {np.mean(eval_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Best Episode Frames\n",
    "\n",
    "if best_frames and len(best_frames) > 0:\n",
    "    print(f\"Visualizing best episode (Episode {best_episode}, Reward: {best_reward:.2f})\")\n",
    "    \n",
    "    # Select frames to display (10 evenly spaced)\n",
    "    n_frames_to_show = 10\n",
    "    step_indices = np.linspace(0, len(best_frames)-1, n_frames_to_show, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    \n",
    "    for ax, idx in zip(axes.flat, step_indices):\n",
    "        ax.imshow(best_frames[idx])\n",
    "        ax.set_title(f\"Step {idx}\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Best Episode Visualization (Reward: {best_reward:.2f})\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['log_dir']}/best_episode_frames.png\", dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No frames captured for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Model Checkpoint Comparison\n",
    "\n",
    "print(\"Comparing model checkpoints...\")\n",
    "\n",
    "# Find all checkpoints\n",
    "checkpoint_files = [\n",
    "    f for f in os.listdir(CONFIG['model_dir']) \n",
    "    if f.startswith('ppo_checkpoint') and f.endswith('.zip')\n",
    "]\n",
    "checkpoint_files.sort(key=lambda x: int(x.split('_')[-2]))  # Sort by step number\n",
    "\n",
    "if len(checkpoint_files) > 0:\n",
    "    print(f\"Found {len(checkpoint_files)} checkpoints\")\n",
    "    \n",
    "    # Evaluate each checkpoint (limit to 5 for speed)\n",
    "    checkpoints_to_eval = checkpoint_files[:5] if len(checkpoint_files) > 5 else checkpoint_files\n",
    "    checkpoint_results = {}\n",
    "    \n",
    "    for cp_file in checkpoints_to_eval:\n",
    "        cp_path = os.path.join(CONFIG['model_dir'], cp_file.replace('.zip', ''))\n",
    "        cp_model = PPO.load(cp_path)\n",
    "        \n",
    "        # Quick evaluation (5 episodes)\n",
    "        mean_reward, std_reward = evaluate_policy(\n",
    "            cp_model, eval_env, n_eval_episodes=5, deterministic=True\n",
    "        )\n",
    "        \n",
    "        checkpoint_results[cp_file] = (mean_reward, std_reward)\n",
    "        print(f\"  {cp_file}: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    # Also evaluate final model\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, eval_env, n_eval_episodes=5, deterministic=True\n",
    "    )\n",
    "    checkpoint_results['ppo_final.zip'] = (mean_reward, std_reward)\n",
    "    print(f\"  ppo_final.zip: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    # Plot checkpoint comparison\n",
    "    if checkpoint_results:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        names = list(checkpoint_results.keys())\n",
    "        means = [v[0] for v in checkpoint_results.values()]\n",
    "        stds = [v[1] for v in checkpoint_results.values()]\n",
    "        \n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(names)))\n",
    "        bars = ax.bar(range(len(names)), means, yerr=stds, capsize=5,\n",
    "                      color=colors, edgecolor='black')\n",
    "        \n",
    "        ax.set_xticks(range(len(names)))\n",
    "        ax.set_xticklabels([n.replace('ppo_checkpoint_', 'CP ').replace('_steps.zip', '') \n",
    "                           for n in names], rotation=45, ha='right')\n",
    "        ax.set_ylabel('Mean Reward')\n",
    "        ax.set_title('Model Performance Across Training')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{CONFIG['log_dir']}/checkpoint_comparison.png\", dpi=150)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No checkpoints found for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps: Applying to Mario Kart\n",
    "\n",
    "Now that you have a working training pipeline, here's how to adapt it for Mario Kart 64:\n",
    "\n",
    "### 1. Swap the Environment\n",
    "\n",
    "Replace `LunarLander-v3` with your custom Mario Kart Gymnasium wrapper:\n",
    "\n",
    "```python\n",
    "CONFIG['env_name'] = 'MarioKart64-v0'  # Your registered environment\n",
    "```\n",
    "\n",
    "### 2. Use CnnPolicy for Image Observations\n",
    "\n",
    "Mario Kart uses screen frames as observations, so switch to CNN:\n",
    "\n",
    "```python\n",
    "model = PPO(\n",
    "    'CnnPolicy',  # Instead of 'MlpPolicy'\n",
    "    env=train_envs,\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Add Reward Wrapper\n",
    "\n",
    "Use the reward functions from Notebook 9:\n",
    "\n",
    "```python\n",
    "from reward_functions import MarioKartRewardV3\n",
    "from gymnasium import Wrapper\n",
    "\n",
    "class RewardWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reward_fn = MarioKartRewardV3()\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        state = self._extract_state(info)\n",
    "        custom_reward = self.reward_fn.compute(state)\n",
    "        return obs, custom_reward, terminated, truncated, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.reward_fn.reset()\n",
    "        return self.env.reset(**kwargs)\n",
    "```\n",
    "\n",
    "### 4. Adjust Hyperparameters\n",
    "\n",
    "Mario Kart likely needs different settings:\n",
    "\n",
    "```python\n",
    "MARIO_KART_CONFIG = {\n",
    "    'env_name': 'MarioKart64-v0',\n",
    "    'policy': 'CnnPolicy',\n",
    "    'total_timesteps': 5_000_000,  # More training needed\n",
    "    'n_envs': 4,  # Fewer envs (higher memory per env)\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'n_steps': 1024,  # Smaller batches\n",
    "    'ent_coef': 0.01,  # Encourage exploration\n",
    "}\n",
    "```\n",
    "\n",
    "### 5. Increase Training Time\n",
    "\n",
    "Complex visual environments need more experience:\n",
    "- LunarLander: ~500K steps\n",
    "- Atari games: ~10M steps\n",
    "- Mario Kart: ~1-10M steps (estimate)\n",
    "\n",
    "### 6. Monitor Carefully\n",
    "\n",
    "Use TensorBoard to watch for:\n",
    "- Reward hacking (high reward but bad driving)\n",
    "- Policy collapse (entropy dropping to zero)\n",
    "- Slow learning (try different reward shaping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "Test your understanding of training at scale:\n",
    "\n",
    "### Question 1\n",
    "Why use vectorized environments (multiple parallel envs)?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "Vectorized environments provide several benefits:\n",
    "- **Faster data collection**: Collect more experience per unit time\n",
    "- **Better gradient estimates**: More diverse samples reduce variance\n",
    "- **Hardware utilization**: Use multiple CPU cores efficiently\n",
    "- **Batch processing**: Neural network forward passes are batched\n",
    "</details>\n",
    "\n",
    "### Question 2\n",
    "What does VecMonitor track and why is it important?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "VecMonitor automatically tracks:\n",
    "- Episode rewards and lengths\n",
    "- Episode completion times\n",
    "- Running statistics\n",
    "\n",
    "It's important because this data is needed for:\n",
    "- TensorBoard logging\n",
    "- Evaluation callbacks\n",
    "- Progress monitoring\n",
    "</details>\n",
    "\n",
    "### Question 3\n",
    "When should you use SubprocVecEnv vs DummyVecEnv?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "**SubprocVecEnv** (multiprocessing):\n",
    "- Faster for CPU-bound environments\n",
    "- Uses more memory (separate process per env)\n",
    "- Best for production training\n",
    "\n",
    "**DummyVecEnv** (single process):\n",
    "- Easier to debug (shared memory space)\n",
    "- Lower memory usage\n",
    "- Better for development and testing\n",
    "</details>\n",
    "\n",
    "### Question 4\n",
    "What TensorBoard metrics might indicate reward hacking?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "Signs of reward hacking:\n",
    "- High `ep_rew_mean` but poor qualitative behavior (need to visualize)\n",
    "- Very short episode lengths despite high rewards\n",
    "- Reward increases but auxiliary metrics (e.g., actual game score) don't improve\n",
    "- Sudden reward spikes without corresponding policy improvement\n",
    "</details>\n",
    "\n",
    "### Question 5\n",
    "Why save checkpoints during training?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "Checkpoints are valuable because:\n",
    "- **Recovery**: Resume training after crashes or interruptions\n",
    "- **Model selection**: Choose the best model (not always the final one)\n",
    "- **Analysis**: Study how the policy evolves over training\n",
    "- **Debugging**: Identify when problems started\n",
    "- **Reproducibility**: Return to any point in training\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Close Environments and Print Summary\n",
    "\n",
    "print(\"Cleaning up...\")\n",
    "\n",
    "# Close environments\n",
    "train_envs.close()\n",
    "eval_env.close()\n",
    "\n",
    "print(\"Environments closed.\")\n",
    "\n",
    "# Summary of saved files\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nSaved Files:\")\n",
    "print(f\"  Models:\")\n",
    "print(f\"    - Final model: {CONFIG['model_dir']}/ppo_final.zip\")\n",
    "print(f\"    - Best model:  {CONFIG['model_dir']}/best_model.zip\")\n",
    "\n",
    "# List checkpoints\n",
    "checkpoints = [f for f in os.listdir(CONFIG['model_dir']) if 'checkpoint' in f]\n",
    "print(f\"    - Checkpoints: {len(checkpoints)} files\")\n",
    "\n",
    "print(f\"\\n  Logs:\")\n",
    "print(f\"    - TensorBoard: {CONFIG['tensorboard_log']}/\")\n",
    "print(f\"    - Evaluations: {CONFIG['log_dir']}/evaluations.npz\")\n",
    "print(f\"    - Plots:       {CONFIG['log_dir']}/*.png\")\n",
    "\n",
    "print(\"\\nTo view TensorBoard logs later:\")\n",
    "print(f\"  tensorboard --logdir {CONFIG['tensorboard_log']}\")\n",
    "\n",
    "print(\"\\nTo load and use the trained model:\")\n",
    "print(f\"  model = PPO.load('{CONFIG['model_dir']}/best_model')\")\n",
    "print(f\"  action, _ = model.predict(observation, deterministic=True)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Congratulations! You've completed the full training pipeline.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "### 1. Training Infrastructure\n",
    "- Configuration management with dictionaries\n",
    "- Vectorized environments for parallel training\n",
    "- VecMonitor for automatic statistics\n",
    "\n",
    "### 2. Monitoring and Logging\n",
    "- TensorBoard integration\n",
    "- Custom callbacks for detailed tracking\n",
    "- Understanding key metrics\n",
    "\n",
    "### 3. Evaluation and Visualization\n",
    "- Checkpoint callbacks for model saving\n",
    "- Evaluation callbacks for best model selection\n",
    "- Episode visualization\n",
    "\n",
    "### 4. Debugging and Diagnosis\n",
    "- Common failure modes\n",
    "- Debugging strategies\n",
    "- Checkpoint comparison\n",
    "\n",
    "### 5. Next Steps\n",
    "- Adapting for Mario Kart (CnnPolicy, reward wrappers)\n",
    "- Scaling up training time\n",
    "- Hyperparameter tuning\n",
    "\n",
    "You now have all the tools to train RL agents at scale!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
