{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 9: Reward Engineering for Mario Kart\n",
    "\n",
    "**The reward function is the heart of reinforcement learning.** It defines what behavior we want our agent to learn. In this notebook, we'll explore the art and science of reward engineering, building progressively more sophisticated reward functions for racing games.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand why reward design is critical for RL success\n",
    "- Learn the difference between sparse and dense rewards\n",
    "- Implement modular, composable reward functions\n",
    "- Build and compare multiple reward function versions\n",
    "- Recognize and avoid common reward hacking pitfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Reward Design Matters\n",
    "\n",
    "In reinforcement learning, **the reward function is the only way we communicate our goals to the agent**. The agent will learn to maximize whatever reward signal we provide - nothing more, nothing less.\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "1. **The agent learns what you reward, not what you want**\n",
    "   - If your reward doesn't capture your true objective, the agent will find unintended shortcuts\n",
    "   \n",
    "2. **Reward shaping can dramatically accelerate learning**\n",
    "   - Good intermediate rewards guide the agent toward the goal\n",
    "   \n",
    "3. **Bad rewards lead to bad behavior**\n",
    "   - \"Reward hacking\" occurs when agents exploit loopholes in the reward function\n",
    "\n",
    "### Racing Game Example\n",
    "\n",
    "Consider a simple reward: `+1` for winning the race, `0` otherwise.\n",
    "\n",
    "**Problems:**\n",
    "- The agent might never experience the `+1` reward during exploration\n",
    "- No feedback about whether it's improving\n",
    "- Learning from such sparse signals is extremely difficult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse vs Dense Rewards\n",
    "\n",
    "### Sparse Rewards\n",
    "Reward is only given at significant events (e.g., winning, scoring, completing a level).\n",
    "\n",
    "**Pros:**\n",
    "- Simple to define\n",
    "- Directly tied to the true objective\n",
    "- Less risk of reward hacking\n",
    "\n",
    "**Cons:**\n",
    "- Hard to learn from (credit assignment problem)\n",
    "- Agent may never discover the reward during exploration\n",
    "- Slow convergence\n",
    "\n",
    "### Dense Rewards\n",
    "Continuous feedback at every timestep based on intermediate progress.\n",
    "\n",
    "**Pros:**\n",
    "- Faster learning\n",
    "- Clear signal for improvement\n",
    "- Easier credit assignment\n",
    "\n",
    "**Cons:**\n",
    "- More complex to design correctly\n",
    "- Risk of reward hacking\n",
    "- May not align with true objective\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Aspect | Sparse Reward | Dense Reward |\n",
    "|--------|---------------|---------------|\n",
    "| **Example** | +100 for completing lap | +1 per checkpoint, +0.1 per speed unit |\n",
    "| **Learning Speed** | Slow | Fast |\n",
    "| **Design Complexity** | Low | High |\n",
    "| **Hacking Risk** | Low | High |\n",
    "| **Credit Assignment** | Difficult | Easy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Shaping and Potential-Based Shaping\n",
    "\n",
    "### What is Reward Shaping?\n",
    "Adding intermediate rewards to guide the agent toward the goal without changing the optimal policy.\n",
    "\n",
    "### Potential-Based Reward Shaping (PBRS)\n",
    "\n",
    "Ng et al. (1999) proved that rewards of the form:\n",
    "\n",
    "$$F(s, s') = \\gamma \\Phi(s') - \\Phi(s)$$\n",
    "\n",
    "where $\\Phi(s)$ is a potential function, **preserve the optimal policy**.\n",
    "\n",
    "### Example: Distance-Based Potential\n",
    "\n",
    "For racing, we could define:\n",
    "$$\\Phi(s) = -\\text{distance\\_to\\_finish}(s)$$\n",
    "\n",
    "This gives positive reward for getting closer to the finish line.\n",
    "\n",
    "### Benefits of PBRS\n",
    "1. **Theoretical guarantee**: Optimal policy unchanged\n",
    "2. **Faster learning**: Dense feedback signal\n",
    "3. **Flexible**: Can incorporate domain knowledge\n",
    "\n",
    "### Practical Considerations\n",
    "- Real-world environments may not have perfect state information\n",
    "- Approximations can still cause subtle policy changes\n",
    "- Magnitude matters: too strong shaping can dominate the true reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Racing Reward Components\n",
    "\n",
    "| Component | Description | Typical Weight | Implementation Notes |\n",
    "|-----------|-------------|----------------|----------------------|\n",
    "| **Speed** | Current velocity normalized by max speed | 0.1 - 0.5 | Encourages fast driving; normalize to [0, 1] |\n",
    "| **Progress** | Change in track checkpoint/position | 0.5 - 1.0 | Main objective; handle lap wraparound |\n",
    "| **Position** | Race position (1st through 8th) | 0.1 - 0.3 | For competitive behavior in multi-agent |\n",
    "| **Collision** | Penalty for hitting walls/karts | -0.1 to -0.5 | Detect via sudden speed drops or collision flag |\n",
    "| **Lap Completion** | Bonus for completing a lap | 5.0 - 100.0 | Sparse milestone reward |\n",
    "| **Time Penalty** | Small negative per timestep | -0.01 to -0.001 | Encourages faster completion |\n",
    "| **Drift Bonus** | Reward for successful drifts | 0.1 - 0.5 | Game-specific mechanic |\n",
    "| **Item Usage** | Reward/penalty for item use | Variable | Context-dependent |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Hacking Pitfalls\n",
    "\n",
    "Reward hacking occurs when agents find unintended ways to maximize reward without achieving the true objective.\n",
    "\n",
    "### Common Racing Game Exploits\n",
    "\n",
    "1. **Spinning in Place**\n",
    "   - **Cause**: Speed reward without direction check\n",
    "   - **Solution**: Reward forward progress, not just speed\n",
    "\n",
    "2. **Driving Backwards**\n",
    "   - **Cause**: Progress reward that can be negative\n",
    "   - **Solution**: Penalize backward movement explicitly\n",
    "\n",
    "3. **Exploiting Shortcuts**\n",
    "   - **Cause**: Only rewarding checkpoints, not actual path\n",
    "   - **Solution**: May be acceptable (creative!) or add path constraints\n",
    "\n",
    "4. **Wall Grinding**\n",
    "   - **Cause**: No collision penalty or too weak\n",
    "   - **Solution**: Add meaningful collision penalties\n",
    "\n",
    "5. **Camping at Bonus Locations**\n",
    "   - **Cause**: High bonus item rewards without completion incentive\n",
    "   - **Solution**: Time penalties, progress requirements\n",
    "\n",
    "6. **Intentional Crashing**\n",
    "   - **Cause**: Reset puts agent in advantageous position\n",
    "   - **Solution**: Penalize deaths/resets heavily\n",
    "\n",
    "### Detection Strategies\n",
    "- Visualize learned policies regularly\n",
    "- Track auxiliary metrics (actual lap times, collisions)\n",
    "- Compare to human performance patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RewardFunction Abstract Base Class\n",
    "\n",
    "class RewardFunction(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for reward functions.\n",
    "    \n",
    "    All reward functions must implement:\n",
    "    - compute(state): Calculate reward for current state\n",
    "    - reset(): Reset internal state for new episode\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute(self, state: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Compute reward given current state.\n",
    "        \n",
    "        Args:\n",
    "            state: Dictionary containing game state information\n",
    "                   Expected keys may include: 'speed', 'checkpoint', 'lap',\n",
    "                   'position', 'collision', etc.\n",
    "        \n",
    "        Returns:\n",
    "            float: Reward value for this timestep\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset internal state for a new episode.\n",
    "        \n",
    "        Called at the start of each episode to clear any\n",
    "        accumulated state (e.g., previous checkpoint).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"RewardFunction base class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpeedReward Class\n",
    "\n",
    "class SpeedReward(RewardFunction):\n",
    "    \"\"\"\n",
    "    Reward based on normalized speed.\n",
    "    \n",
    "    Encourages the agent to drive fast by giving reward\n",
    "    proportional to current speed normalized by maximum speed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_speed: float = 100.0, weight: float = 1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_speed: Maximum possible speed for normalization\n",
    "            weight: Multiplier for the reward\n",
    "        \"\"\"\n",
    "        self.max_speed = max_speed\n",
    "        self.weight = weight\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Compute speed-based reward.\n",
    "        \n",
    "        Returns value in range [0, weight] based on current speed.\n",
    "        \"\"\"\n",
    "        speed = state.get('speed', 0)\n",
    "        # Normalize speed to [0, 1] and apply weight\n",
    "        normalized = min(speed / self.max_speed, 1.0)\n",
    "        return self.weight * normalized\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"No internal state to reset.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test SpeedReward\n",
    "speed_reward = SpeedReward(max_speed=100.0, weight=0.5)\n",
    "test_states = [\n",
    "    {'speed': 0},\n",
    "    {'speed': 50},\n",
    "    {'speed': 100},\n",
    "    {'speed': 120},  # Over max\n",
    "]\n",
    "\n",
    "print(\"SpeedReward Test:\")\n",
    "for state in test_states:\n",
    "    reward = speed_reward.compute(state)\n",
    "    print(f\"  Speed {state['speed']:3d} -> Reward {reward:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProgressReward Class\n",
    "\n",
    "class ProgressReward(RewardFunction):\n",
    "    \"\"\"\n",
    "    Reward based on checkpoint progress with lap wrap handling.\n",
    "    \n",
    "    Tracks progress through checkpoints and rewards forward movement.\n",
    "    Handles the wrap-around when completing a lap (checkpoint N -> 0).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, total_checkpoints: int = 100, weight: float = 1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            total_checkpoints: Number of checkpoints per lap\n",
    "            weight: Multiplier for the reward\n",
    "        \"\"\"\n",
    "        self.total_checkpoints = total_checkpoints\n",
    "        self.weight = weight\n",
    "        self.last_checkpoint = 0\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Compute progress-based reward.\n",
    "        \n",
    "        Returns reward proportional to checkpoints passed since last call.\n",
    "        Handles lap wrap-around correctly.\n",
    "        \"\"\"\n",
    "        current_cp = state.get('checkpoint', 0)\n",
    "        delta = current_cp - self.last_checkpoint\n",
    "        \n",
    "        # Handle lap wrap (e.g., going from checkpoint 99 to 0)\n",
    "        if delta < -self.total_checkpoints // 2:\n",
    "            delta += self.total_checkpoints\n",
    "        # Handle backward movement (going from 0 to 99 backwards)\n",
    "        elif delta > self.total_checkpoints // 2:\n",
    "            delta -= self.total_checkpoints\n",
    "        \n",
    "        self.last_checkpoint = current_cp\n",
    "        return self.weight * delta\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset checkpoint tracking for new episode.\"\"\"\n",
    "        self.last_checkpoint = 0\n",
    "\n",
    "# Test ProgressReward\n",
    "progress_reward = ProgressReward(total_checkpoints=100, weight=1.0)\n",
    "\n",
    "test_sequence = [\n",
    "    {'checkpoint': 0},   # Start\n",
    "    {'checkpoint': 5},   # Normal forward\n",
    "    {'checkpoint': 10},  # More forward\n",
    "    {'checkpoint': 98},  # Near lap end\n",
    "    {'checkpoint': 2},   # Lap wrap!\n",
    "    {'checkpoint': 1},   # Slight backward\n",
    "]\n",
    "\n",
    "print(\"ProgressReward Test (wrap at 100):\")\n",
    "progress_reward.reset()\n",
    "for i, state in enumerate(test_sequence):\n",
    "    reward = progress_reward.compute(state)\n",
    "    print(f\"  Step {i}: Checkpoint {state['checkpoint']:2d} -> Reward {reward:+.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CompositeReward Class\n",
    "\n",
    "class CompositeReward(RewardFunction):\n",
    "    \"\"\"\n",
    "    Combine multiple reward functions with weights.\n",
    "    \n",
    "    Allows building complex reward functions from simpler components.\n",
    "    Each component has its own weight for fine-tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reward_fns: List[Tuple[RewardFunction, float]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reward_fns: List of (RewardFunction, weight) tuples\n",
    "        \"\"\"\n",
    "        self.reward_fns = reward_fns\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Compute weighted sum of all component rewards.\n",
    "        \"\"\"\n",
    "        total = 0.0\n",
    "        for fn, weight in self.reward_fns:\n",
    "            total += weight * fn.compute(state)\n",
    "        return total\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset all component reward functions.\"\"\"\n",
    "        for fn, _ in self.reward_fns:\n",
    "            fn.reset()\n",
    "    \n",
    "    def compute_breakdown(self, state: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get individual rewards from each component (for debugging).\n",
    "        \"\"\"\n",
    "        breakdown = {}\n",
    "        for fn, weight in self.reward_fns:\n",
    "            name = fn.__class__.__name__\n",
    "            breakdown[name] = weight * fn.compute(state)\n",
    "        return breakdown\n",
    "\n",
    "# Test CompositeReward\n",
    "composite = CompositeReward([\n",
    "    (SpeedReward(max_speed=100), 0.3),\n",
    "    (ProgressReward(total_checkpoints=100), 1.0),\n",
    "])\n",
    "\n",
    "composite.reset()\n",
    "test_state = {'speed': 80, 'checkpoint': 5}\n",
    "reward = composite.compute(test_state)\n",
    "print(f\"Composite Reward Test:\")\n",
    "print(f\"  State: {test_state}\")\n",
    "print(f\"  Total Reward: {reward:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MarioKartRewardV1: Speed + Progress\n",
    "\n",
    "class MarioKartRewardV1(RewardFunction):\n",
    "    \"\"\"\n",
    "    Version 1: Basic speed and progress rewards.\n",
    "    \n",
    "    Combines:\n",
    "    - Speed reward (weight 0.3): Encourages fast driving\n",
    "    - Progress reward (weight 1.0): Main objective - move forward\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_speed: float = 100.0, total_checkpoints: int = 100):\n",
    "        self.max_speed = max_speed\n",
    "        self.total_checkpoints = total_checkpoints\n",
    "        \n",
    "        self.speed_reward = SpeedReward(max_speed, weight=0.3)\n",
    "        self.progress_reward = ProgressReward(total_checkpoints, weight=1.0)\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        speed_r = self.speed_reward.compute(state)\n",
    "        progress_r = self.progress_reward.compute(state)\n",
    "        return speed_r + progress_r\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.speed_reward.reset()\n",
    "        self.progress_reward.reset()\n",
    "\n",
    "print(\"MarioKartRewardV1 defined: Speed + Progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MarioKartRewardV2: V1 + Collision Penalty\n",
    "\n",
    "class MarioKartRewardV2(MarioKartRewardV1):\n",
    "    \"\"\"\n",
    "    Version 2: V1 + Collision penalty.\n",
    "    \n",
    "    Adds collision detection via sudden speed drops.\n",
    "    When speed drops dramatically, we assume a collision occurred.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_speed: float = 100.0, total_checkpoints: int = 100,\n",
    "                 collision_threshold: float = 20.0, collision_penalty: float = -0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            collision_threshold: Speed drop that indicates collision\n",
    "            collision_penalty: Penalty applied on collision\n",
    "        \"\"\"\n",
    "        super().__init__(max_speed, total_checkpoints)\n",
    "        self.collision_threshold = collision_threshold\n",
    "        self.collision_penalty = collision_penalty\n",
    "        self.last_speed = 0\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        # Get base reward from V1\n",
    "        base_reward = super().compute(state)\n",
    "        \n",
    "        # Detect collision via sudden speed drop\n",
    "        current_speed = state.get('speed', 0)\n",
    "        speed_drop = self.last_speed - current_speed\n",
    "        \n",
    "        collision_r = 0.0\n",
    "        if speed_drop > self.collision_threshold:\n",
    "            collision_r = self.collision_penalty\n",
    "        \n",
    "        self.last_speed = current_speed\n",
    "        return base_reward + collision_r\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self.last_speed = 0\n",
    "\n",
    "print(\"MarioKartRewardV2 defined: V1 + Collision Penalty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MarioKartRewardV3: V2 + Lap Bonus + Time Penalty\n",
    "\n",
    "class MarioKartRewardV3(MarioKartRewardV2):\n",
    "    \"\"\"\n",
    "    Version 3: V2 + Lap bonus + Time penalty.\n",
    "    \n",
    "    Adds:\n",
    "    - Lap completion bonus: Big reward for finishing laps\n",
    "    - Time penalty: Small constant penalty to encourage speed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_speed: float = 100.0, total_checkpoints: int = 100,\n",
    "                 collision_threshold: float = 20.0, collision_penalty: float = -0.5,\n",
    "                 lap_bonus: float = 10.0, time_penalty: float = -0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lap_bonus: Reward for completing a lap\n",
    "            time_penalty: Penalty per timestep (encourages faster completion)\n",
    "        \"\"\"\n",
    "        super().__init__(max_speed, total_checkpoints, \n",
    "                        collision_threshold, collision_penalty)\n",
    "        self.lap_bonus = lap_bonus\n",
    "        self.time_penalty = time_penalty\n",
    "        self.last_lap = 0\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        # Get base reward from V2\n",
    "        base_reward = super().compute(state)\n",
    "        \n",
    "        # Lap completion bonus\n",
    "        current_lap = state.get('lap', 0)\n",
    "        lap_r = 0.0\n",
    "        if current_lap > self.last_lap:\n",
    "            lap_r = self.lap_bonus\n",
    "        self.last_lap = current_lap\n",
    "        \n",
    "        # Time penalty (constant per step)\n",
    "        time_r = self.time_penalty\n",
    "        \n",
    "        return base_reward + lap_r + time_r\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self.last_lap = 0\n",
    "\n",
    "print(\"MarioKartRewardV3 defined: V2 + Lap Bonus + Time Penalty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Simulated State Sequence\n",
    "\n",
    "def generate_simulated_episode(n_steps: int = 50, seed: int = 42) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate a realistic simulated racing episode.\n",
    "    \n",
    "    Simulates:\n",
    "    - Acceleration from start\n",
    "    - Cruising at high speed\n",
    "    - Collision event (speed drops)\n",
    "    - Recovery\n",
    "    - Lap completion\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    states = []\n",
    "    \n",
    "    speed = 0\n",
    "    checkpoint = 0\n",
    "    lap = 0\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # Acceleration phase (first 10 steps)\n",
    "        if i < 10:\n",
    "            speed = min(speed + 10, 80)\n",
    "        # Collision at step 25\n",
    "        elif i == 25:\n",
    "            speed = 20  # Sudden drop\n",
    "        # Recovery after collision\n",
    "        elif 25 < i < 35:\n",
    "            speed = min(speed + 8, 80)\n",
    "        # Normal cruising with small variations\n",
    "        else:\n",
    "            speed = max(60, min(90, speed + np.random.randint(-5, 6)))\n",
    "        \n",
    "        # Progress through checkpoints\n",
    "        if speed > 30:\n",
    "            checkpoint = (checkpoint + 2) % 100\n",
    "        else:\n",
    "            checkpoint = (checkpoint + 1) % 100\n",
    "        \n",
    "        # Lap completion (when checkpoint wraps)\n",
    "        if checkpoint < 5 and i > 0 and states[-1]['checkpoint'] > 95:\n",
    "            lap += 1\n",
    "        \n",
    "        states.append({\n",
    "            'speed': speed,\n",
    "            'checkpoint': checkpoint,\n",
    "            'lap': lap\n",
    "        })\n",
    "    \n",
    "    return states\n",
    "\n",
    "# Generate test episode\n",
    "test_episode = generate_simulated_episode(n_steps=50)\n",
    "\n",
    "# Evaluate all three reward versions\n",
    "reward_v1 = MarioKartRewardV1()\n",
    "reward_v2 = MarioKartRewardV2()\n",
    "reward_v3 = MarioKartRewardV3()\n",
    "\n",
    "reward_v1.reset()\n",
    "reward_v2.reset()\n",
    "reward_v3.reset()\n",
    "\n",
    "rewards_v1 = []\n",
    "rewards_v2 = []\n",
    "rewards_v3 = []\n",
    "\n",
    "for state in test_episode:\n",
    "    rewards_v1.append(reward_v1.compute(state))\n",
    "    rewards_v2.append(reward_v2.compute(state))\n",
    "    rewards_v3.append(reward_v3.compute(state))\n",
    "\n",
    "print(f\"Episode Summary (50 steps):\")\n",
    "print(f\"  V1 Total Reward: {sum(rewards_v1):.2f}\")\n",
    "print(f\"  V2 Total Reward: {sum(rewards_v2):.2f}\")\n",
    "print(f\"  V3 Total Reward: {sum(rewards_v3):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Comparison of V1/V2/V3\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: State information\n",
    "ax1 = axes[0, 0]\n",
    "steps = range(len(test_episode))\n",
    "speeds = [s['speed'] for s in test_episode]\n",
    "checkpoints = [s['checkpoint'] for s in test_episode]\n",
    "\n",
    "ax1.plot(steps, speeds, 'b-', label='Speed', linewidth=2)\n",
    "ax1.axvline(x=25, color='r', linestyle='--', alpha=0.5, label='Collision Event')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Speed')\n",
    "ax1.set_title('Simulated Episode: Speed Profile')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Per-step rewards\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(steps, rewards_v1, 'g-', label='V1: Speed+Progress', alpha=0.8)\n",
    "ax2.plot(steps, rewards_v2, 'b-', label='V2: +Collision', alpha=0.8)\n",
    "ax2.plot(steps, rewards_v3, 'r-', label='V3: +Lap+Time', alpha=0.8)\n",
    "ax2.axvline(x=25, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Reward')\n",
    "ax2.set_title('Per-Step Rewards by Version')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Cumulative rewards\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(steps, np.cumsum(rewards_v1), 'g-', label='V1', linewidth=2)\n",
    "ax3.plot(steps, np.cumsum(rewards_v2), 'b-', label='V2', linewidth=2)\n",
    "ax3.plot(steps, np.cumsum(rewards_v3), 'r-', label='V3', linewidth=2)\n",
    "ax3.set_xlabel('Step')\n",
    "ax3.set_ylabel('Cumulative Reward')\n",
    "ax3.set_title('Cumulative Rewards Over Episode')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Total reward comparison\n",
    "ax4 = axes[1, 1]\n",
    "versions = ['V1\\n(Speed+Progress)', 'V2\\n(+Collision)', 'V3\\n(+Lap+Time)']\n",
    "totals = [sum(rewards_v1), sum(rewards_v2), sum(rewards_v3)]\n",
    "colors = ['green', 'blue', 'red']\n",
    "bars = ax4.bar(versions, totals, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_ylabel('Total Reward')\n",
    "ax4.set_title('Total Episode Reward Comparison')\n",
    "for bar, total in zip(bars, totals):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{total:.1f}', ha='center', va='bottom', fontsize=12)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- V2 shows a penalty spike at step 25 (collision detected)\")\n",
    "print(\"- V3 has slightly lower rewards due to constant time penalty\")\n",
    "print(\"- All versions reward forward progress similarly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended Simulation: 200 Steps with Multiple Events\n",
    "\n",
    "def generate_extended_episode(n_steps: int = 200, seed: int = 123) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate a longer episode with multiple events:\n",
    "    - Multiple collisions\n",
    "    - Multiple lap completions\n",
    "    - Speed variations\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    states = []\n",
    "    \n",
    "    speed = 0\n",
    "    checkpoint = 0\n",
    "    lap = 0\n",
    "    collision_steps = [30, 80, 150]  # Collision events\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # Handle collisions\n",
    "        if i in collision_steps:\n",
    "            speed = max(10, speed - 50)  # Big speed drop\n",
    "        # Acceleration\n",
    "        elif speed < 80:\n",
    "            speed = min(speed + 5, 80)\n",
    "        # Normal variation\n",
    "        else:\n",
    "            speed = max(60, min(90, speed + np.random.randint(-3, 4)))\n",
    "        \n",
    "        # Progress (faster when speed is higher)\n",
    "        progress_rate = 1 + int(speed / 40)\n",
    "        old_checkpoint = checkpoint\n",
    "        checkpoint = (checkpoint + progress_rate) % 100\n",
    "        \n",
    "        # Detect lap completion\n",
    "        if checkpoint < old_checkpoint:\n",
    "            lap += 1\n",
    "        \n",
    "        states.append({\n",
    "            'speed': speed,\n",
    "            'checkpoint': checkpoint,\n",
    "            'lap': lap\n",
    "        })\n",
    "    \n",
    "    return states\n",
    "\n",
    "# Generate extended episode\n",
    "extended_episode = generate_extended_episode(n_steps=200)\n",
    "\n",
    "# Reset and evaluate\n",
    "reward_v1 = MarioKartRewardV1()\n",
    "reward_v2 = MarioKartRewardV2()\n",
    "reward_v3 = MarioKartRewardV3()\n",
    "\n",
    "reward_v1.reset()\n",
    "reward_v2.reset()\n",
    "reward_v3.reset()\n",
    "\n",
    "ext_rewards_v1 = []\n",
    "ext_rewards_v2 = []\n",
    "ext_rewards_v3 = []\n",
    "\n",
    "for state in extended_episode:\n",
    "    ext_rewards_v1.append(reward_v1.compute(state))\n",
    "    ext_rewards_v2.append(reward_v2.compute(state))\n",
    "    ext_rewards_v3.append(reward_v3.compute(state))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "steps = range(len(extended_episode))\n",
    "speeds = [s['speed'] for s in extended_episode]\n",
    "laps = [s['lap'] for s in extended_episode]\n",
    "\n",
    "# Speed and lap profile\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(steps, speeds, 'b-', label='Speed', linewidth=1.5)\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(steps, laps, 'g--', label='Lap', linewidth=2)\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Speed', color='blue')\n",
    "ax1_twin.set_ylabel('Lap', color='green')\n",
    "ax1.set_title('Extended Episode: Speed and Lap Progress')\n",
    "for cs in [30, 80, 150]:\n",
    "    ax1.axvline(x=cs, color='red', linestyle='--', alpha=0.3)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative rewards\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(steps, np.cumsum(ext_rewards_v1), 'g-', label='V1', linewidth=2)\n",
    "ax2.plot(steps, np.cumsum(ext_rewards_v2), 'b-', label='V2', linewidth=2)\n",
    "ax2.plot(steps, np.cumsum(ext_rewards_v3), 'r-', label='V3', linewidth=2)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Cumulative Reward')\n",
    "ax2.set_title('Cumulative Rewards (200 Steps)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling average reward\n",
    "ax3 = axes[1, 0]\n",
    "window = 20\n",
    "rolling_v1 = np.convolve(ext_rewards_v1, np.ones(window)/window, mode='valid')\n",
    "rolling_v2 = np.convolve(ext_rewards_v2, np.ones(window)/window, mode='valid')\n",
    "rolling_v3 = np.convolve(ext_rewards_v3, np.ones(window)/window, mode='valid')\n",
    "ax3.plot(rolling_v1, 'g-', label='V1', alpha=0.8)\n",
    "ax3.plot(rolling_v2, 'b-', label='V2', alpha=0.8)\n",
    "ax3.plot(rolling_v3, 'r-', label='V3', alpha=0.8)\n",
    "ax3.set_xlabel('Step')\n",
    "ax3.set_ylabel('Rolling Avg Reward (20 steps)')\n",
    "ax3.set_title('Rolling Average Reward')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Final comparison\n",
    "ax4 = axes[1, 1]\n",
    "versions = ['V1', 'V2', 'V3']\n",
    "totals = [sum(ext_rewards_v1), sum(ext_rewards_v2), sum(ext_rewards_v3)]\n",
    "colors = ['green', 'blue', 'red']\n",
    "bars = ax4.bar(versions, totals, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_ylabel('Total Reward')\n",
    "ax4.set_title('Total Episode Reward (200 Steps)')\n",
    "for bar, total in zip(bars, totals):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{total:.1f}', ha='center', va='bottom', fontsize=12)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "final_lap = extended_episode[-1]['lap']\n",
    "print(f\"\\nExtended Episode Summary:\")\n",
    "print(f\"  Total Steps: 200\")\n",
    "print(f\"  Laps Completed: {final_lap}\")\n",
    "print(f\"  Collisions: 3\")\n",
    "print(f\"\\nTotal Rewards:\")\n",
    "print(f\"  V1 (Speed+Progress):     {sum(ext_rewards_v1):.2f}\")\n",
    "print(f\"  V2 (+Collision Penalty): {sum(ext_rewards_v2):.2f}\")\n",
    "print(f\"  V3 (+Lap Bonus+Time):    {sum(ext_rewards_v3):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Design Your Own Reward Function\n",
    "\n",
    "Create `MarioKartRewardV4` that extends V3 with a **position-based reward** for competitive racing.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. Add a `position` field to the state (1-8, where 1 is first place)\n",
    "2. Implement position-based reward:\n",
    "   - Higher reward for better positions\n",
    "   - Bonus for overtaking (improving position)\n",
    "   - Penalty for being overtaken\n",
    "\n",
    "### Hints\n",
    "\n",
    "```python\n",
    "class MarioKartRewardV4(MarioKartRewardV3):\n",
    "    def __init__(self, ..., position_weight=0.2):\n",
    "        super().__init__(...)\n",
    "        self.position_weight = position_weight\n",
    "        self.last_position = 4  # Start mid-pack\n",
    "    \n",
    "    def compute(self, state):\n",
    "        base_reward = super().compute(state)\n",
    "        position = state.get('position', 4)\n",
    "        \n",
    "        # Position reward: 1st place = 1.0, 8th place = 0.0\n",
    "        position_r = (8 - position) / 7 * self.position_weight\n",
    "        \n",
    "        # Overtake bonus/penalty\n",
    "        # ... your implementation here ...\n",
    "        \n",
    "        return base_reward + position_r + overtake_r\n",
    "```\n",
    "\n",
    "### Discussion Questions\n",
    "\n",
    "1. How might position rewards interact with other reward components?\n",
    "2. Could position rewards lead to aggressive/dangerous driving behavior?\n",
    "3. Should the position reward scale differently for 1st vs 8th place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "Test your understanding of reward engineering concepts:\n",
    "\n",
    "### Question 1\n",
    "Why can sparse rewards be problematic for RL training?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "Sparse rewards make learning difficult because:\n",
    "- The agent may rarely or never experience the reward during exploration\n",
    "- Credit assignment is hard (which actions led to the reward?)\n",
    "- Learning is slow with infrequent feedback\n",
    "- The agent gets no guidance on whether it's improving\n",
    "</details>\n",
    "\n",
    "### Question 2\n",
    "What is potential-based reward shaping and why is it useful?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "Potential-based reward shaping adds intermediate rewards of the form F(s,s') = gamma * Phi(s') - Phi(s). It's useful because:\n",
    "- It preserves the optimal policy (theoretically guaranteed)\n",
    "- Provides dense feedback to accelerate learning\n",
    "- Allows incorporating domain knowledge (e.g., distance to goal)\n",
    "</details>\n",
    "\n",
    "### Question 3\n",
    "How does MarioKartRewardV2 detect collisions?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "V2 detects collisions by monitoring sudden speed drops. When the current speed is much lower than the previous speed (drop exceeds `collision_threshold`), it assumes a collision occurred and applies the `collision_penalty`. This is a heuristic approach that works when direct collision information isn't available.\n",
    "</details>\n",
    "\n",
    "### Question 4\n",
    "Why include a time penalty in MarioKartRewardV3?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "The time penalty (small negative reward per timestep) serves several purposes:\n",
    "- Encourages faster completion of laps/races\n",
    "- Prevents the agent from \"stalling\" or moving very slowly\n",
    "- Creates urgency that better matches the racing objective\n",
    "- Helps distinguish between completing quickly vs slowly\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reward_functions.py Module\n",
    "\n",
    "reward_module_code = '''\n",
    "\"\"\"\n",
    "Reward Functions for Mario Kart Reinforcement Learning\n",
    "\n",
    "This module provides modular, composable reward functions for training\n",
    "RL agents on racing games. Each reward function follows the RewardFunction\n",
    "abstract base class interface.\n",
    "\n",
    "Classes:\n",
    "    RewardFunction: Abstract base class\n",
    "    SpeedReward: Reward based on normalized speed\n",
    "    ProgressReward: Reward based on checkpoint progress\n",
    "    CompositeReward: Combine multiple reward functions\n",
    "    MarioKartRewardV1: Speed + Progress\n",
    "    MarioKartRewardV2: V1 + Collision penalty\n",
    "    MarioKartRewardV3: V2 + Lap bonus + Time penalty\n",
    "\n",
    "Example:\n",
    "    >>> reward_fn = MarioKartRewardV3()\n",
    "    >>> reward_fn.reset()\n",
    "    >>> state = {\\'speed\\': 80, \\'checkpoint\\': 50, \\'lap\\': 1}\n",
    "    >>> reward = reward_fn.compute(state)\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "class RewardFunction(ABC):\n",
    "    \"\"\"Abstract base class for reward functions.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute(self, state: Dict) -> float:\n",
    "        \"\"\"Compute reward given current state.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset internal state for new episode.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class SpeedReward(RewardFunction):\n",
    "    \"\"\"Reward based on normalized speed.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_speed: float = 100.0, weight: float = 1.0):\n",
    "        self.max_speed = max_speed\n",
    "        self.weight = weight\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        speed = state.get(\\'speed\\', 0)\n",
    "        normalized = min(speed / self.max_speed, 1.0)\n",
    "        return self.weight * normalized\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ProgressReward(RewardFunction):\n",
    "    \"\"\"Reward based on checkpoint progress with lap wrap handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, total_checkpoints: int = 100, weight: float = 1.0):\n",
    "        self.total_checkpoints = total_checkpoints\n",
    "        self.weight = weight\n",
    "        self.last_checkpoint = 0\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        current_cp = state.get(\\'checkpoint\\', 0)\n",
    "        delta = current_cp - self.last_checkpoint\n",
    "        \n",
    "        if delta < -self.total_checkpoints // 2:\n",
    "            delta += self.total_checkpoints\n",
    "        elif delta > self.total_checkpoints // 2:\n",
    "            delta -= self.total_checkpoints\n",
    "        \n",
    "        self.last_checkpoint = current_cp\n",
    "        return self.weight * delta\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.last_checkpoint = 0\n",
    "\n",
    "\n",
    "class CompositeReward(RewardFunction):\n",
    "    \"\"\"Combine multiple reward functions with weights.\"\"\"\n",
    "    \n",
    "    def __init__(self, reward_fns: List[Tuple[RewardFunction, float]]):\n",
    "        self.reward_fns = reward_fns\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        total = 0.0\n",
    "        for fn, weight in self.reward_fns:\n",
    "            total += weight * fn.compute(state)\n",
    "        return total\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        for fn, _ in self.reward_fns:\n",
    "            fn.reset()\n",
    "\n",
    "\n",
    "class MarioKartRewardV1(RewardFunction):\n",
    "    \"\"\"V1: Speed + Progress rewards.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_speed: float = 100.0, total_checkpoints: int = 100):\n",
    "        self.speed_reward = SpeedReward(max_speed, weight=0.3)\n",
    "        self.progress_reward = ProgressReward(total_checkpoints, weight=1.0)\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        return (self.speed_reward.compute(state) + \n",
    "                self.progress_reward.compute(state))\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.speed_reward.reset()\n",
    "        self.progress_reward.reset()\n",
    "\n",
    "\n",
    "class MarioKartRewardV2(MarioKartRewardV1):\n",
    "    \"\"\"V2: V1 + Collision penalty (via speed drop detection).\"\"\"\n",
    "    \n",
    "    def __init__(self, max_speed: float = 100.0, total_checkpoints: int = 100,\n",
    "                 collision_threshold: float = 20.0, collision_penalty: float = -0.5):\n",
    "        super().__init__(max_speed, total_checkpoints)\n",
    "        self.collision_threshold = collision_threshold\n",
    "        self.collision_penalty = collision_penalty\n",
    "        self.last_speed = 0\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        base_reward = super().compute(state)\n",
    "        current_speed = state.get(\\'speed\\', 0)\n",
    "        speed_drop = self.last_speed - current_speed\n",
    "        \n",
    "        collision_r = self.collision_penalty if speed_drop > self.collision_threshold else 0\n",
    "        self.last_speed = current_speed\n",
    "        return base_reward + collision_r\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self.last_speed = 0\n",
    "\n",
    "\n",
    "class MarioKartRewardV3(MarioKartRewardV2):\n",
    "    \"\"\"V3: V2 + Lap bonus + Time penalty.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_speed: float = 100.0, total_checkpoints: int = 100,\n",
    "                 collision_threshold: float = 20.0, collision_penalty: float = -0.5,\n",
    "                 lap_bonus: float = 10.0, time_penalty: float = -0.01):\n",
    "        super().__init__(max_speed, total_checkpoints, \n",
    "                        collision_threshold, collision_penalty)\n",
    "        self.lap_bonus = lap_bonus\n",
    "        self.time_penalty = time_penalty\n",
    "        self.last_lap = 0\n",
    "    \n",
    "    def compute(self, state: Dict) -> float:\n",
    "        base_reward = super().compute(state)\n",
    "        current_lap = state.get(\\'lap\\', 0)\n",
    "        \n",
    "        lap_r = self.lap_bonus if current_lap > self.last_lap else 0\n",
    "        self.last_lap = current_lap\n",
    "        \n",
    "        return base_reward + lap_r + self.time_penalty\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self.last_lap = 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick test\n",
    "    reward_fn = MarioKartRewardV3()\n",
    "    reward_fn.reset()\n",
    "    \n",
    "    test_states = [\n",
    "        {\\'speed\\': 50, \\'checkpoint\\': 0, \\'lap\\': 0},\n",
    "        {\\'speed\\': 80, \\'checkpoint\\': 5, \\'lap\\': 0},\n",
    "        {\\'speed\\': 80, \\'checkpoint\\': 10, \\'lap\\': 0},\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing MarioKartRewardV3:\")\n",
    "    for state in test_states:\n",
    "        r = reward_fn.compute(state)\n",
    "        print(f\"  State: {state} -> Reward: {r:.3f}\")\n",
    "'''\n",
    "\n",
    "# Write the module\n",
    "with open('reward_functions.py', 'w') as f:\n",
    "    f.write(reward_module_code)\n",
    "\n",
    "print(\"Saved: reward_functions.py\")\n",
    "\n",
    "# Verify import works\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"reward_functions\", \"reward_functions.py\")\n",
    "reward_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(reward_module)\n",
    "\n",
    "print(\"\\nModule contents:\")\n",
    "print([name for name in dir(reward_module) if not name.startswith('_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **Reward Design Fundamentals**\n",
    "   - The reward function defines what behavior the agent learns\n",
    "   - Sparse vs dense rewards trade off simplicity for learning speed\n",
    "\n",
    "2. **Reward Shaping**\n",
    "   - Potential-based shaping can accelerate learning\n",
    "   - Must be careful not to change the optimal policy\n",
    "\n",
    "3. **Modular Reward Architecture**\n",
    "   - Abstract base class for consistent interface\n",
    "   - Composable components (speed, progress, etc.)\n",
    "   - Versioned rewards for iterative improvement\n",
    "\n",
    "4. **Racing-Specific Considerations**\n",
    "   - Progress tracking with lap wraparound\n",
    "   - Collision detection via speed drops\n",
    "   - Balance between speed and safety\n",
    "\n",
    "5. **Avoiding Pitfalls**\n",
    "   - Reward hacking detection and prevention\n",
    "   - Importance of visualization and monitoring\n",
    "\n",
    "### Next Steps\n",
    "- Implement V4 with position rewards\n",
    "- Integrate with actual Mario Kart environment\n",
    "- Experiment with different weight configurations\n",
    "- Use reward_functions.py in training scripts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
