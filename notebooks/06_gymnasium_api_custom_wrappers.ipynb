{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 6: Gymnasium API and Custom Wrappers\n",
    "\n",
    "In this notebook, we dive deep into the Gymnasium API and learn how to create custom wrappers to modify environment behavior. Wrappers are a powerful tool for preprocessing observations, shaping rewards, and adding functionality without modifying the original environment.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master the Gymnasium environment API (reset, step, render, close)\n",
    "- Understand observation and action spaces\n",
    "- Learn the wrapper design pattern\n",
    "- Implement custom wrappers for various purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q \"gymnasium[classic-control]\" numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Wrapper, ObservationWrapper, ActionWrapper, RewardWrapper\n",
    "from gymnasium.wrappers import TimeLimit, RecordEpisodeStatistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from typing import Any, SupportsFloat\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Theory: Gymnasium API Overview\n",
    "\n",
    "Gymnasium provides a standardized interface for reinforcement learning environments. Every environment implements these core methods:\n",
    "\n",
    "### Core Methods\n",
    "\n",
    "#### `reset(seed=None, options=None) -> (observation, info)`\n",
    "- Resets the environment to an initial state\n",
    "- `seed`: Optional random seed for reproducibility\n",
    "- `options`: Optional dict for environment-specific reset parameters\n",
    "- Returns: Initial observation and info dict\n",
    "\n",
    "#### `step(action) -> (observation, reward, terminated, truncated, info)`\n",
    "- Executes one timestep in the environment\n",
    "- `action`: The action to take (must be in action_space)\n",
    "- Returns: 5-tuple with new state, reward, and episode status\n",
    "\n",
    "#### `render() -> RenderFrame | list[RenderFrame] | None`\n",
    "- Renders the environment for visualization\n",
    "- Mode determined by `render_mode` in `gym.make()`\n",
    "- Common modes: \"human\", \"rgb_array\", \"ansi\"\n",
    "\n",
    "#### `close()`\n",
    "- Cleans up resources (display windows, files, etc.)\n",
    "- Should always be called when done with an environment\n",
    "\n",
    "### Environment Properties\n",
    "\n",
    "```python\n",
    "env.observation_space  # Space of valid observations\n",
    "env.action_space       # Space of valid actions\n",
    "env.spec               # Environment specification\n",
    "env.metadata           # Environment metadata (render modes, etc.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Theory: Step Return Values\n",
    "\n",
    "The `step()` method returns a 5-tuple. Understanding each component is crucial:\n",
    "\n",
    "### `(observation, reward, terminated, truncated, info)`\n",
    "\n",
    "| Return Value | Type | Description |\n",
    "|-------------|------|-------------|\n",
    "| `observation` | Depends on env | The new state after taking the action |\n",
    "| `reward` | `float` | Immediate reward from the transition |\n",
    "| `terminated` | `bool` | True if episode ended due to environment rules (goal reached, failure state) |\n",
    "| `truncated` | `bool` | True if episode ended due to time limit or other external condition |\n",
    "| `info` | `dict` | Additional diagnostic information |\n",
    "\n",
    "### Terminated vs Truncated\n",
    "\n",
    "**Terminated (Natural ending)**:\n",
    "- CartPole: Pole fell beyond 12 degrees\n",
    "- MountainCar: Car reached the goal\n",
    "- Game over states\n",
    "\n",
    "**Truncated (Artificial ending)**:\n",
    "- Maximum timesteps reached (TimeLimit wrapper)\n",
    "- Training budget exhausted\n",
    "- External interruption\n",
    "\n",
    "### Why the distinction matters\n",
    "\n",
    "```python\n",
    "# For bootstrapping in value-based methods:\n",
    "if terminated:\n",
    "    # True terminal state - no future rewards\n",
    "    target = reward\n",
    "elif truncated:\n",
    "    # Episode cut short - should still bootstrap\n",
    "    target = reward + gamma * V(next_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Theory: Spaces\n",
    "\n",
    "Gymnasium uses `Space` objects to define valid observations and actions. Understanding spaces is essential for designing agents and wrappers.\n",
    "\n",
    "### Common Space Types\n",
    "\n",
    "#### `Box` - Continuous values in a bounded range\n",
    "```python\n",
    "# Single continuous value in [0, 1]\n",
    "Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "\n",
    "# 84x84 grayscale image with pixels in [0, 255]\n",
    "Box(low=0, high=255, shape=(84, 84), dtype=np.uint8)\n",
    "\n",
    "# 4D observation with different bounds per dimension\n",
    "Box(low=np.array([-1, 0, -5, -np.inf]), \n",
    "    high=np.array([1, 10, 5, np.inf]), \n",
    "    dtype=np.float32)\n",
    "```\n",
    "\n",
    "#### `Discrete` - Single integer from {0, 1, ..., n-1}\n",
    "```python\n",
    "# 4 possible actions: 0, 1, 2, 3\n",
    "Discrete(4)\n",
    "```\n",
    "\n",
    "#### `MultiDiscrete` - Multiple discrete values\n",
    "```python\n",
    "# Two discrete variables: first in {0,1,2}, second in {0,1,2,3,4}\n",
    "MultiDiscrete([3, 5])\n",
    "```\n",
    "\n",
    "#### `Dict` - Dictionary of spaces\n",
    "```python\n",
    "Dict({\n",
    "    'position': Box(low=-10, high=10, shape=(2,)),\n",
    "    'velocity': Box(low=-1, high=1, shape=(2,)),\n",
    "    'inventory': Discrete(10)\n",
    "})\n",
    "```\n",
    "\n",
    "#### `Tuple` - Tuple of spaces\n",
    "```python\n",
    "Tuple((Discrete(3), Box(low=0, high=1, shape=(2,))))\n",
    "```\n",
    "\n",
    "### Space Methods\n",
    "```python\n",
    "space.sample()          # Random sample from the space\n",
    "space.contains(x)       # Check if x is valid\n",
    "space.shape             # Shape of samples (for Box)\n",
    "space.n                 # Number of elements (for Discrete)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore Different Environment Space Types\n",
    "\n",
    "Let's examine real environments with discrete and continuous spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole - Discrete action space\n",
    "print(\"=\"*60)\n",
    "print(\"CARTPOLE - DISCRETE ACTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cartpole = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"\\nObservation Space: {cartpole.observation_space}\")\n",
    "print(f\"  Type: {type(cartpole.observation_space).__name__}\")\n",
    "print(f\"  Shape: {cartpole.observation_space.shape}\")\n",
    "print(f\"  Low bounds: {cartpole.observation_space.low}\")\n",
    "print(f\"  High bounds: {cartpole.observation_space.high}\")\n",
    "print(f\"  Dtype: {cartpole.observation_space.dtype}\")\n",
    "\n",
    "print(f\"\\nAction Space: {cartpole.action_space}\")\n",
    "print(f\"  Type: {type(cartpole.action_space).__name__}\")\n",
    "print(f\"  Number of actions: {cartpole.action_space.n}\")\n",
    "print(f\"  Actions: 0=Push Left, 1=Push Right\")\n",
    "\n",
    "# Sample observation and action\n",
    "print(f\"\\nSample observation: {cartpole.observation_space.sample()}\")\n",
    "print(f\"Sample action: {cartpole.action_space.sample()}\")\n",
    "\n",
    "# Describe observation components\n",
    "print(\"\\nObservation components:\")\n",
    "print(\"  [0] Cart position (-4.8 to 4.8)\")\n",
    "print(\"  [1] Cart velocity (-inf to inf)\")\n",
    "print(\"  [2] Pole angle (-0.418 to 0.418 radians)\")\n",
    "print(\"  [3] Pole angular velocity (-inf to inf)\")\n",
    "\n",
    "cartpole.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendulum - Continuous action space\n",
    "print(\"=\"*60)\n",
    "print(\"PENDULUM - CONTINUOUS ACTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pendulum = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "print(f\"\\nObservation Space: {pendulum.observation_space}\")\n",
    "print(f\"  Type: {type(pendulum.observation_space).__name__}\")\n",
    "print(f\"  Shape: {pendulum.observation_space.shape}\")\n",
    "print(f\"  Low bounds: {pendulum.observation_space.low}\")\n",
    "print(f\"  High bounds: {pendulum.observation_space.high}\")\n",
    "\n",
    "print(f\"\\nAction Space: {pendulum.action_space}\")\n",
    "print(f\"  Type: {type(pendulum.action_space).__name__}\")\n",
    "print(f\"  Shape: {pendulum.action_space.shape}\")\n",
    "print(f\"  Low bound: {pendulum.action_space.low}\")\n",
    "print(f\"  High bound: {pendulum.action_space.high}\")\n",
    "print(f\"  Action: Torque applied to pendulum (continuous from -2 to 2)\")\n",
    "\n",
    "# Sample observation and action\n",
    "print(f\"\\nSample observation: {pendulum.observation_space.sample()}\")\n",
    "print(f\"Sample action: {pendulum.action_space.sample()}\")\n",
    "\n",
    "print(\"\\nObservation components:\")\n",
    "print(\"  [0] cos(theta) - x-coordinate of pendulum end\")\n",
    "print(\"  [1] sin(theta) - y-coordinate of pendulum end\")\n",
    "print(\"  [2] Angular velocity (theta_dot)\")\n",
    "\n",
    "pendulum.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create and explore some custom spaces\n",
    "print(\"=\"*60)\n",
    "print(\"CUSTOM SPACE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# MultiDiscrete space\n",
    "multi_discrete = spaces.MultiDiscrete([3, 5, 2])  # 3 actions, 5 actions, 2 actions\n",
    "print(f\"\\nMultiDiscrete([3, 5, 2]):\")\n",
    "print(f\"  Sample: {multi_discrete.sample()}\")\n",
    "print(f\"  nvec: {multi_discrete.nvec}\")\n",
    "print(f\"  Valid: [0-2, 0-4, 0-1]\")\n",
    "\n",
    "# Dict space\n",
    "dict_space = spaces.Dict({\n",
    "    'position': spaces.Box(low=-10, high=10, shape=(2,), dtype=np.float32),\n",
    "    'velocity': spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32),\n",
    "    'has_key': spaces.Discrete(2)\n",
    "})\n",
    "print(f\"\\nDict space sample:\")\n",
    "sample = dict_space.sample()\n",
    "for key, value in sample.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Tuple space\n",
    "tuple_space = spaces.Tuple((\n",
    "    spaces.Discrete(4),\n",
    "    spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
    "))\n",
    "print(f\"\\nTuple space sample: {tuple_space.sample()}\")\n",
    "\n",
    "# Check if values are valid\n",
    "print(\"\\nSpace containment checks:\")\n",
    "print(f\"  Discrete(4).contains(2): {spaces.Discrete(4).contains(2)}\")\n",
    "print(f\"  Discrete(4).contains(5): {spaces.Discrete(4).contains(5)}\")\n",
    "box = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)\n",
    "print(f\"  Box[0,1].contains([0.5, 0.5]): {box.contains(np.array([0.5, 0.5], dtype=np.float32))}\")\n",
    "print(f\"  Box[0,1].contains([1.5, 0.5]): {box.contains(np.array([1.5, 0.5], dtype=np.float32))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Theory: The Wrapper Pattern\n",
    "\n",
    "### What is a Wrapper?\n",
    "\n",
    "A wrapper is a design pattern that allows you to modify an environment's behavior without changing its original code. Wrappers \"wrap around\" an environment and intercept/modify its methods.\n",
    "\n",
    "### Why Use Wrappers?\n",
    "\n",
    "1. **Modularity**: Keep environment modifications separate and reusable\n",
    "2. **Composability**: Stack multiple wrappers for complex transformations\n",
    "3. **Separation of concerns**: Preprocessing, logging, reward shaping in separate components\n",
    "4. **Flexibility**: Easy to add/remove modifications without changing base environment\n",
    "\n",
    "### Wrapper Hierarchy\n",
    "\n",
    "```\n",
    "gymnasium.Wrapper (base class)\n",
    "    |\n",
    "    |-- ObservationWrapper  (modify observations)\n",
    "    |-- ActionWrapper       (modify actions)\n",
    "    |-- RewardWrapper       (modify rewards)\n",
    "```\n",
    "\n",
    "### How Wrappers Work\n",
    "\n",
    "```python\n",
    "class Wrapper(gym.Env):\n",
    "    def __init__(self, env):\n",
    "        self.env = env  # Store the wrapped environment\n",
    "        # Inherit spaces from wrapped env\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)  # Delegate to wrapped env\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "```\n",
    "\n",
    "### Stacking Wrappers\n",
    "\n",
    "```python\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = WrapperA(env)  # First wrapper\n",
    "env = WrapperB(env)  # Second wrapper (wraps WrapperA)\n",
    "env = WrapperC(env)  # Third wrapper (wraps WrapperB)\n",
    "\n",
    "# Call order for step():\n",
    "# User -> WrapperC -> WrapperB -> WrapperA -> CartPole\n",
    "# Return order:\n",
    "# CartPole -> WrapperA -> WrapperB -> WrapperC -> User\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implement LoggingWrapper\n",
    "\n",
    "Let's create a wrapper that tracks episode statistics like rewards, lengths, and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingWrapper(Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper that logs episode statistics.\n",
    "    \n",
    "    Tracks:\n",
    "    - Episode rewards (total per episode)\n",
    "    - Episode lengths (steps per episode)\n",
    "    - Episode count\n",
    "    - Step count\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: gym.Env, window_size: int = 100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: The environment to wrap\n",
    "            window_size: Number of recent episodes to track for moving averages\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Statistics storage\n",
    "        self.episode_rewards = deque(maxlen=window_size)\n",
    "        self.episode_lengths = deque(maxlen=window_size)\n",
    "        \n",
    "        # Current episode tracking\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.current_episode_length = 0\n",
    "        \n",
    "        # Global counters\n",
    "        self.total_episodes = 0\n",
    "        self.total_steps = 0\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset the environment and episode tracking.\"\"\"\n",
    "        # Reset current episode trackers\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.current_episode_length = 0\n",
    "        \n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Step the environment and update statistics.\"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Update current episode stats\n",
    "        self.current_episode_reward += reward\n",
    "        self.current_episode_length += 1\n",
    "        self.total_steps += 1\n",
    "        \n",
    "        # Check if episode ended\n",
    "        if terminated or truncated:\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_lengths.append(self.current_episode_length)\n",
    "            self.total_episodes += 1\n",
    "            \n",
    "            # Add episode info to info dict\n",
    "            info['episode'] = {\n",
    "                'reward': self.current_episode_reward,\n",
    "                'length': self.current_episode_length,\n",
    "                'episode_num': self.total_episodes\n",
    "            }\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def get_statistics(self) -> dict:\n",
    "        \"\"\"Return current statistics.\"\"\"\n",
    "        stats = {\n",
    "            'total_episodes': self.total_episodes,\n",
    "            'total_steps': self.total_steps,\n",
    "        }\n",
    "        \n",
    "        if len(self.episode_rewards) > 0:\n",
    "            stats.update({\n",
    "                'mean_reward': np.mean(self.episode_rewards),\n",
    "                'std_reward': np.std(self.episode_rewards),\n",
    "                'min_reward': np.min(self.episode_rewards),\n",
    "                'max_reward': np.max(self.episode_rewards),\n",
    "                'mean_length': np.mean(self.episode_lengths),\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        \"\"\"Pretty print current statistics.\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"LOGGING WRAPPER STATISTICS\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(f\"Total episodes: {stats['total_episodes']}\")\n",
    "        print(f\"Total steps: {stats['total_steps']}\")\n",
    "        if 'mean_reward' in stats:\n",
    "            print(f\"Mean reward (last {len(self.episode_rewards)}): {stats['mean_reward']:.2f} +/- {stats['std_reward']:.2f}\")\n",
    "            print(f\"Reward range: [{stats['min_reward']:.2f}, {stats['max_reward']:.2f}]\")\n",
    "            print(f\"Mean episode length: {stats['mean_length']:.1f}\")\n",
    "\n",
    "# Test the LoggingWrapper\n",
    "print(\"Testing LoggingWrapper...\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = LoggingWrapper(env)\n",
    "\n",
    "# Run a few episodes\n",
    "for episode in range(5):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: Reward = {info['episode']['reward']:.1f}, Length = {info['episode']['length']}\")\n",
    "\n",
    "env.print_statistics()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Implement ScaledRewardWrapper\n",
    "\n",
    "A RewardWrapper subclass that scales rewards by a constant factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledRewardWrapper(RewardWrapper):\n",
    "    \"\"\"\n",
    "    Scales rewards by a constant factor.\n",
    "    \n",
    "    Useful for:\n",
    "    - Normalizing rewards across different environments\n",
    "    - Adjusting reward magnitude for learning stability\n",
    "    - Implementing reward shaping\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: gym.Env, scale: float = 1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: The environment to wrap\n",
    "            scale: Factor to multiply rewards by\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.scale = scale\n",
    "    \n",
    "    def reward(self, reward: SupportsFloat) -> SupportsFloat:\n",
    "        \"\"\"\n",
    "        Transform the reward.\n",
    "        \n",
    "        This method is called automatically by RewardWrapper.step()\n",
    "        \"\"\"\n",
    "        return reward * self.scale\n",
    "\n",
    "# Test the ScaledRewardWrapper\n",
    "print(\"Testing ScaledRewardWrapper...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create environments with different scales\n",
    "env_normal = gym.make(\"CartPole-v1\")\n",
    "env_scaled = ScaledRewardWrapper(gym.make(\"CartPole-v1\"), scale=0.01)\n",
    "\n",
    "# Compare rewards\n",
    "obs_normal, _ = env_normal.reset(seed=42)\n",
    "obs_scaled, _ = env_scaled.reset(seed=42)\n",
    "\n",
    "print(\"Taking same action in both environments:\")\n",
    "for step in range(5):\n",
    "    action = 1  # Push right\n",
    "    \n",
    "    _, reward_normal, _, _, _ = env_normal.step(action)\n",
    "    _, reward_scaled, _, _, _ = env_scaled.step(action)\n",
    "    \n",
    "    print(f\"  Step {step + 1}: Normal reward = {reward_normal}, Scaled reward = {reward_scaled}\")\n",
    "\n",
    "print(f\"\\nScale factor: {env_scaled.scale}\")\n",
    "print(\"Notice how scaled rewards are 0.01x the normal rewards.\")\n",
    "\n",
    "env_normal.close()\n",
    "env_scaled.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Implement NormalizeObservationWrapper\n",
    "\n",
    "An ObservationWrapper subclass that normalizes observations to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeObservationWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Normalizes observations using running mean and standard deviation.\n",
    "    \n",
    "    This wrapper maintains running statistics and normalizes observations\n",
    "    to have approximately zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: gym.Env, epsilon: float = 1e-8, clip: float = 10.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: The environment to wrap\n",
    "            epsilon: Small value to prevent division by zero\n",
    "            clip: Clip normalized observations to [-clip, clip]\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "        self.clip = clip\n",
    "        \n",
    "        # Running statistics\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.running_mean = np.zeros(obs_shape, dtype=np.float64)\n",
    "        self.running_var = np.ones(obs_shape, dtype=np.float64)\n",
    "        self.count = 0\n",
    "        \n",
    "        # Update observation space to reflect normalized range\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-clip,\n",
    "            high=clip,\n",
    "            shape=obs_shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize the observation.\n",
    "        \n",
    "        This method is called automatically by ObservationWrapper.step() and reset()\n",
    "        \"\"\"\n",
    "        # Update running statistics (Welford's online algorithm)\n",
    "        self.count += 1\n",
    "        delta = observation - self.running_mean\n",
    "        self.running_mean += delta / self.count\n",
    "        delta2 = observation - self.running_mean\n",
    "        self.running_var += (delta * delta2 - self.running_var) / self.count\n",
    "        \n",
    "        # Normalize\n",
    "        normalized = (observation - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Clip to prevent extreme values\n",
    "        return np.clip(normalized, -self.clip, self.clip).astype(np.float32)\n",
    "    \n",
    "    def get_statistics(self) -> dict:\n",
    "        \"\"\"Return current normalization statistics.\"\"\"\n",
    "        return {\n",
    "            'mean': self.running_mean.copy(),\n",
    "            'std': np.sqrt(self.running_var),\n",
    "            'count': self.count\n",
    "        }\n",
    "\n",
    "# Test the NormalizeObservationWrapper\n",
    "print(\"Testing NormalizeObservationWrapper...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "env = NormalizeObservationWrapper(gym.make(\"CartPole-v1\"))\n",
    "\n",
    "# Collect some observations\n",
    "observations = []\n",
    "obs, _ = env.reset()\n",
    "observations.append(obs)\n",
    "\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()\n",
    "    obs, _, terminated, truncated, _ = env.step(action)\n",
    "    observations.append(obs)\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "observations = np.array(observations)\n",
    "\n",
    "print(f\"\\nNormalized observation statistics:\")\n",
    "print(f\"  Mean: {observations.mean(axis=0)}\")\n",
    "print(f\"  Std:  {observations.std(axis=0)}\")\n",
    "print(f\"  Min:  {observations.min(axis=0)}\")\n",
    "print(f\"  Max:  {observations.max(axis=0)}\")\n",
    "\n",
    "print(f\"\\nObservation space bounds: [{env.observation_space.low[0]}, {env.observation_space.high[0]}]\")\n",
    "print(\"Notice observations are centered around 0 with ~unit variance.\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Implement FrameSkipWrapper\n",
    "\n",
    "A wrapper that repeats actions for multiple frames (common in Atari and robotics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSkipWrapper(Wrapper):\n",
    "    \"\"\"\n",
    "    Repeats actions for a specified number of frames.\n",
    "    \n",
    "    Benefits:\n",
    "    - Reduces decision frequency (faster training)\n",
    "    - Encourages temporally extended actions\n",
    "    - Accumulates rewards over skipped frames\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: gym.Env, skip: int = 4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: The environment to wrap\n",
    "            skip: Number of times to repeat each action\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Repeat action for `skip` frames, accumulating reward.\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        for _ in range(self.skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Stop early if episode ends\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        return obs, total_reward, terminated, truncated, info\n",
    "\n",
    "# Test the FrameSkipWrapper\n",
    "print(\"Testing FrameSkipWrapper...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "env_normal = gym.make(\"CartPole-v1\")\n",
    "env_skip = FrameSkipWrapper(gym.make(\"CartPole-v1\"), skip=4)\n",
    "\n",
    "# Compare step counts\n",
    "def run_episode(env, seed=42):\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = 1  # Always push right for consistency\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    return total_reward, steps\n",
    "\n",
    "reward_normal, steps_normal = run_episode(env_normal)\n",
    "reward_skip, steps_skip = run_episode(env_skip)\n",
    "\n",
    "print(f\"Normal environment:\")\n",
    "print(f\"  Steps taken: {steps_normal}\")\n",
    "print(f\"  Total reward: {reward_normal}\")\n",
    "\n",
    "print(f\"\\nFrame-skip (skip=4) environment:\")\n",
    "print(f\"  Steps taken: {steps_skip}\")\n",
    "print(f\"  Total reward: {reward_skip}\")\n",
    "print(f\"  Effective frames: ~{steps_skip * 4}\")\n",
    "\n",
    "print(f\"\\nStep reduction: {steps_normal / steps_skip:.1f}x fewer decisions\")\n",
    "print(\"Note: Rewards are similar because frame-skip accumulates rewards.\")\n",
    "\n",
    "env_normal.close()\n",
    "env_skip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Demonstrate Wrapper Composition\n",
    "\n",
    "Let's stack multiple wrappers together to see how they compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"WRAPPER COMPOSITION DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a heavily wrapped environment\n",
    "# Order matters! Wrappers are applied from inside out\n",
    "\n",
    "# Start with base environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "print(f\"\\n1. Base environment: {type(env).__name__}\")\n",
    "\n",
    "# Add frame skip (reduces step frequency)\n",
    "env = FrameSkipWrapper(env, skip=2)\n",
    "print(f\"2. Add FrameSkipWrapper (skip=2): {type(env).__name__}\")\n",
    "\n",
    "# Add observation normalization\n",
    "env = NormalizeObservationWrapper(env)\n",
    "print(f\"3. Add NormalizeObservationWrapper: {type(env).__name__}\")\n",
    "\n",
    "# Add reward scaling\n",
    "env = ScaledRewardWrapper(env, scale=0.1)\n",
    "print(f\"4. Add ScaledRewardWrapper (scale=0.1): {type(env).__name__}\")\n",
    "\n",
    "# Add logging\n",
    "env = LoggingWrapper(env)\n",
    "print(f\"5. Add LoggingWrapper: {type(env).__name__}\")\n",
    "\n",
    "# Show the wrapper chain\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"Wrapper chain (outer to inner):\")\n",
    "current = env\n",
    "depth = 0\n",
    "while hasattr(current, 'env'):\n",
    "    print(f\"  {'  ' * depth}{type(current).__name__}\")\n",
    "    current = current.env\n",
    "    depth += 1\n",
    "print(f\"  {'  ' * depth}{type(current).__name__} (base)\")\n",
    "\n",
    "# Test the composed environment\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"Testing composed environment:\")\n",
    "\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(f\"\\nInitial observation (normalized): {obs}\")\n",
    "\n",
    "# Run one episode\n",
    "done = False\n",
    "step_count = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    step_count += 1\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    if step_count <= 3:\n",
    "        print(f\"\\nStep {step_count}:\")\n",
    "        print(f\"  Observation: {obs}\")\n",
    "        print(f\"  Reward (scaled): {reward}\")\n",
    "\n",
    "print(f\"\\nEpisode complete!\")\n",
    "if 'episode' in info:\n",
    "    print(f\"Total reward: {info['episode']['reward']:.2f}\")\n",
    "    print(f\"Total steps: {info['episode']['length']}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Wrapped Environment\n",
    "\n",
    "Let's run multiple episodes with our composed environment and analyze the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our full wrapper stack\n",
    "def create_wrapped_env(seed=None):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env = FrameSkipWrapper(env, skip=2)\n",
    "    env = NormalizeObservationWrapper(env)\n",
    "    env = ScaledRewardWrapper(env, scale=0.1)\n",
    "    env = LoggingWrapper(env)\n",
    "    return env\n",
    "\n",
    "env = create_wrapped_env()\n",
    "\n",
    "print(\"Running 20 episodes with wrapped environment...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(20):\n",
    "    obs, _ = env.reset(seed=episode)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Simple policy: move in direction of pole lean\n",
    "        # obs[2] is pole angle (negative = leaning left)\n",
    "        action = 1 if obs[2] > 0 else 0\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    episode_rewards.append(info['episode']['reward'])\n",
    "    episode_lengths.append(info['episode']['length'])\n",
    "    \n",
    "    if (episode + 1) % 5 == 0:\n",
    "        print(f\"Episode {episode + 1}: Reward = {info['episode']['reward']:.2f}, Length = {info['episode']['length']}\")\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(episode_rewards, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=np.mean(episode_rewards), color='r', linestyle='--', label=f'Mean: {np.mean(episode_rewards):.2f}')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Episode Reward (scaled)')\n",
    "axes[0].set_title('Episode Rewards')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(episode_lengths, 'g-', linewidth=2)\n",
    "axes[1].axhline(y=np.mean(episode_lengths), color='r', linestyle='--', label=f'Mean: {np.mean(episode_lengths):.1f}')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Episode Length (wrapper steps)')\n",
    "axes[1].set_title('Episode Lengths')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wrapper_test_results.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics from LoggingWrapper\n",
    "env.print_statistics()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Built-in Gymnasium Wrappers\n",
    "\n",
    "Gymnasium provides many useful built-in wrappers. Let's explore some important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BUILT-IN GYMNASIUM WRAPPERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. TimeLimit - Truncates episodes after max steps\n",
    "print(\"\\n1. TimeLimit Wrapper\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "# Create environment with custom time limit\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = TimeLimit(env, max_episode_steps=50)  # Override default 500\n",
    "\n",
    "obs, _ = env.reset(seed=42)\n",
    "steps = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    obs, _, terminated, truncated, _ = env.step(1)  # Always push right\n",
    "    steps += 1\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"  Episode ended after {steps} steps\")\n",
    "print(f\"  Terminated (natural): {terminated}\")\n",
    "print(f\"  Truncated (time limit): {truncated}\")\n",
    "env.close()\n",
    "\n",
    "# 2. RecordEpisodeStatistics - Tracks episode stats automatically\n",
    "print(\"\\n2. RecordEpisodeStatistics Wrapper\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = RecordEpisodeStatistics(env, buffer_length=10)\n",
    "\n",
    "# Run a few episodes\n",
    "for ep in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        obs, _, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    # Episode stats are in info when episode ends\n",
    "    print(f\"  Episode {ep + 1}: r={info['episode']['r']:.1f}, l={info['episode']['l']}, t={info['episode']['t']:.2f}s\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. More built-in wrappers demonstration\n",
    "print(\"\\n3. Other Useful Built-in Wrappers\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from gymnasium.wrappers import (\n",
    "    FlattenObservation,\n",
    "    GrayscaleObservation,\n",
    "    ResizeObservation,\n",
    "    ClipAction,\n",
    "    RescaleAction,\n",
    ")\n",
    "\n",
    "print(\"\"\"\n",
    "Available wrappers for different purposes:\n",
    "\n",
    "OBSERVATION WRAPPERS:\n",
    "  - FlattenObservation: Flattens dict/tuple observations\n",
    "  - GrayscaleObservation: Converts RGB to grayscale (for images)\n",
    "  - ResizeObservation: Resizes image observations\n",
    "  - NormalizeObservation: Normalizes observations (running stats)\n",
    "  - FrameStack: Stacks multiple frames (built into SB3)\n",
    "\n",
    "ACTION WRAPPERS:\n",
    "  - ClipAction: Clips continuous actions to valid range\n",
    "  - RescaleAction: Rescales action space to new range\n",
    "\n",
    "REWARD WRAPPERS:\n",
    "  - ClipReward: Clips rewards to a range\n",
    "  - NormalizeReward: Normalizes rewards (running stats)\n",
    "\n",
    "UTILITY WRAPPERS:\n",
    "  - TimeLimit: Truncates episodes after N steps\n",
    "  - RecordEpisodeStatistics: Tracks reward/length stats\n",
    "  - RecordVideo: Records episodes as video files\n",
    "  - AutoResetWrapper: Automatically resets on done\n",
    "\"\"\")\n",
    "\n",
    "# Example: RescaleAction\n",
    "print(\"\\nExample: RescaleAction with Pendulum\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "print(f\"Original action space: {env.action_space}\")\n",
    "print(f\"  Range: [{env.action_space.low[0]}, {env.action_space.high[0]}]\")\n",
    "\n",
    "# Rescale to [-1, 1]\n",
    "env_rescaled = RescaleAction(env, min_action=-1.0, max_action=1.0)\n",
    "print(f\"\\nRescaled action space: {env_rescaled.action_space}\")\n",
    "print(f\"  Range: [{env_rescaled.action_space.low[0]}, {env_rescaled.action_space.high[0]}]\")\n",
    "\n",
    "# Action 0.5 in rescaled space becomes 1.0 in original space\n",
    "print(\"\\nAction mapping (rescaled -> original):\")\n",
    "print(\"  -1.0 -> -2.0\")\n",
    "print(\"   0.0 ->  0.0\")\n",
    "print(\"   1.0 ->  2.0\")\n",
    "\n",
    "env.close()\n",
    "env_rescaled.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Quiz: Test Your Understanding\n",
    "\n",
    "Answer these questions to check your understanding of the Gymnasium API and wrappers.\n",
    "\n",
    "---\n",
    "\n",
    "**Question 1**: What is the difference between `terminated` and `truncated` in the step return?\n",
    "\n",
    "A) They mean the same thing - the episode is over  \n",
    "B) `terminated` means natural ending (goal/failure), `truncated` means artificial ending (time limit)  \n",
    "C) `truncated` is for training, `terminated` is for evaluation  \n",
    "D) `terminated` is for discrete actions, `truncated` is for continuous actions\n",
    "\n",
    "---\n",
    "\n",
    "**Question 2**: Which space type would you use for a robot arm with 6 continuous joint angles, each between -pi and pi?\n",
    "\n",
    "A) `Discrete(6)`  \n",
    "B) `MultiDiscrete([6])`  \n",
    "C) `Box(low=-pi, high=pi, shape=(6,))`  \n",
    "D) `Tuple((Discrete(6), Discrete(6)))`\n",
    "\n",
    "---\n",
    "\n",
    "**Question 3**: When stacking wrappers, which wrapper processes the action first?\n",
    "\n",
    "A) The innermost (closest to base environment)  \n",
    "B) The outermost (furthest from base environment)  \n",
    "C) All wrappers process simultaneously  \n",
    "D) It depends on the wrapper type\n",
    "\n",
    "---\n",
    "\n",
    "**Question 4**: Which parent class should you use to create a wrapper that modifies observations?\n",
    "\n",
    "A) `gym.Wrapper`  \n",
    "B) `gym.ObservationWrapper`  \n",
    "C) `gym.ActionWrapper`  \n",
    "D) `gym.RewardWrapper`\n",
    "\n",
    "---\n",
    "\n",
    "**Question 5**: Why is frame skipping useful in reinforcement learning?\n",
    "\n",
    "A) It makes the game graphics look better  \n",
    "B) It reduces decision frequency and encourages temporally extended actions  \n",
    "C) It increases the observation space size  \n",
    "D) It is only used for debugging\n",
    "\n",
    "---\n",
    "\n",
    "### Answers\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answers</summary>\n",
    "\n",
    "1. **B** - `terminated` indicates the episode ended due to the environment's rules (like reaching a goal or failure state), while `truncated` indicates it ended due to external factors like a time limit. This distinction matters for value bootstrapping in RL algorithms.\n",
    "\n",
    "2. **C** - `Box` is the correct choice for continuous values with defined bounds. `Box(low=-pi, high=pi, shape=(6,))` creates a 6-dimensional continuous space where each dimension is bounded between -pi and pi.\n",
    "\n",
    "3. **B** - The outermost wrapper receives the action first. The call chain goes: User -> Outer wrapper -> ... -> Inner wrapper -> Base environment. Return values go in reverse order.\n",
    "\n",
    "4. **B** - `ObservationWrapper` is designed specifically for modifying observations. You only need to implement the `observation()` method, and the wrapper handles calling it at the right times.\n",
    "\n",
    "5. **B** - Frame skipping reduces the number of decisions the agent needs to make (4x fewer with skip=4) and encourages actions that have effects over multiple timesteps, which can speed up training and improve performance.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any remaining environments\n",
    "import gc\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKPOINT 6 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Gymnasium provides a standardized API: reset(), step(), render(), close()\")\n",
    "print(\"2. step() returns (obs, reward, terminated, truncated, info)\")\n",
    "print(\"3. Spaces define valid observations/actions (Box, Discrete, etc.)\")\n",
    "print(\"4. Wrappers modify environment behavior without changing source code\")\n",
    "print(\"5. Specialized wrappers exist: ObservationWrapper, RewardWrapper, ActionWrapper\")\n",
    "print(\"6. Wrappers can be composed/stacked for complex transformations\")\n",
    "print(\"\\nNext: Apply these concepts to build custom N64 Mario Kart environments!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
